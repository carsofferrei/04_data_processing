{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carsofferrei/04_data_processing/blob/main/spark/examples/01-rdds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA_wQSmLd9z"
      },
      "source": [
        "# Data Collections (RDDs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "ca53be2f-52c9-40ab-d5c8-d81ee7b51c37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "637HFw00T3LP"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local').appName('Spark Course').config('spark.ui.port', '4050').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WW76P6DpLpx"
      },
      "outputs": [],
      "source": [
        "# diff between SparkSession and SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SqwZjdL9MnL6"
      },
      "outputs": [],
      "source": [
        "sc = SparkSession.getActiveSession().sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGG7R76hMZ3E"
      },
      "source": [
        "# RDDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DZaxZW0bMZ3F"
      },
      "outputs": [],
      "source": [
        "data = [1, 2, 3, 4, 5]\n",
        "distData = sc.parallelize(data)\n",
        "\n",
        "### Parallelized collections are created by calling SparkContext’s parallelize method on an existing iterable or collection in your driver program\n",
        "### The elements of the collection are copied to form a distributed dataset that can be operated on in parallel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a8eCChgrMZ3H"
      },
      "outputs": [],
      "source": [
        "### Run parallel operations\n",
        "distData.reduce(lambda a, b: a + b)\n",
        "\n",
        "### One important parameter for parallel collections is the number of partitions to cut the dataset into\n",
        "### Spark will run one task for each partition of the cluster\n",
        "### Typically you want 2-4 partitions for each CPU in your cluster\n",
        "### Normally, Spark tries to set the number of partitions automatically based on your cluster\n",
        "### However, you can also set it manually\n",
        "\n",
        "distData = sc.parallelize(data, 10) #10 partições"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8UzHNVzynNm-"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/files/ #comando para criar diretorios Linux\n",
        "text = \"red yellow white green\"\n",
        "\n",
        "text_file = open(\"/content/files/data.txt\", \"w\")\n",
        "text_file.write(text)\n",
        "text_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5qs9KNmJMZ3I"
      },
      "outputs": [],
      "source": [
        "### Read from external datasets\n",
        "distFile = sc.textFile(\"data.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1ftjIsEMZ3J"
      },
      "outputs": [],
      "source": [
        "### RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset\n",
        "\n",
        "### Example:\n",
        "### map is a transformation that passes each dataset element through a function and returns a new RDD representing the results\n",
        "### reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJr1dWvFMZ3J",
        "outputId": "ae3e6fe1-059b-4112-a456-54d6672035d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n"
          ]
        }
      ],
      "source": [
        "### All transformations in Spark are lazy\n",
        "### The transformations are only computed when an action requires a result to be returned to the driver program\n",
        "\n",
        "lines = sc.textFile(\"/content/files/data.txt\")\n",
        "lineLengths = lines.map(lambda s: len(s))\n",
        "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
        "print(totalLength)\n",
        "\n",
        "### The first line defines a base RDD from an external file.\n",
        "### lines is merely a pointer to the file\n",
        "### The second line defines lineLengths as the result of a map transformation\n",
        "### lineLengths is not immediately computed, due to laziness\n",
        "### reducer is an actions so Spark breaks the computation into tasks to run on separate machines\n",
        "### and each machine runs both its part of the map and a local reduction, returning only its answer to the driver program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKflP9NxpweG",
        "outputId": "cea32767-fe30-4fa8-c089-e032982b9a57"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['red yellow white green']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "lines.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFgcn2llperu",
        "outputId": "5b51ba00-baa0-4100-ed8d-ba0da1e97a45"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[8] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "### if we want to use lineLenghts again\n",
        "lineLengths.persist()\n",
        "### before the reduce, which would cause lineLengths to be saved in memory after the first time it is computed.\n",
        "#conseguimos determinar o StorageLevel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lineLengths.cache()"
      ],
      "metadata": {
        "id": "dZ9ihlbh1mRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lineLengths.is_cached"
      ],
      "metadata": {
        "id": "icM9GTFr1uMn",
        "outputId": "64504167-ec57-4397-fed9-32670b4919e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}