{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carsofferrei/04_data_processing/blob/main/spark/examples/06-write_partitioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA_wQSmLd9z"
      },
      "source": [
        "# Write\n",
        "- .write\n",
        "- .format (parquet, csv, json)\n",
        "- options\n",
        "- spark.sql.sources.partitionOverwriteMode dynamic\n",
        "\n",
        "# Write Mode\n",
        "- overwrite - The overwrite mode is used to overwrite the existing file, alternatively, you can use SaveMode.Overwrite\n",
        "- append - To add the data to the existing file, alternatively, you can use SaveMode.Append\n",
        "- ignore - Ignores write operation when the file already exists, alternatively, you can use SaveMode.Ignore.\n",
        "- errorifexists or error - This is a default option when the file already exists, it returns an error, alternatively, you can use SaveMode.ErrorIfExists.\n",
        "\n",
        "# Partitioning\n",
        "Process to organize the data into multiple chunks based on some criteria.\n",
        "Partitions are organized in sub-folders.\n",
        "Partitioning improves performance in Spark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "a5fd5043-4634-474f-b2c0-6508186d94b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "637HFw00T3LP"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local').appName('Spark Course').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj3Cg2riVX3m"
      },
      "source": [
        "# Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83BBHcNJDmw4",
        "outputId": "fc6bb44a-a387-48d6-c262-3ade83e8938c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-33.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Downloading Faker-33.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-33.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z-caHS2MVX3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f948828b-7055-4365-f0be-9fb2b3b37b25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+--------------------------+----------+--------------------------+------------------+----------------------+\n",
            "|address                                            |date                      |dob       |email                     |name              |phone                 |\n",
            "+---------------------------------------------------+--------------------------+----------+--------------------------+------------------+----------------------+\n",
            "|USNS Conway\\nFPO AE 90390                          |2024-05-02 22:32:08.059274|2000-09-09|kathleenwhite@example.net |Brian Ramirez     |547.878.6454          |\n",
            "|23306 Richards Loaf Apt. 472\\nSeanport, OK 99390   |2024-05-03 07:52:11.502189|2005-04-29|caldwelljoanne@example.com|Danielle Myers    |751.218.3248          |\n",
            "|5752 Oconnor Stravenue\\nNorth David, NV 34467      |2024-05-04 17:05:22.245215|1922-06-21|michelle55@example.com    |Wesley Watkins PhD|+1-600-737-1209x55300 |\n",
            "|6686 Burns Forks\\nNormanfort, NV 27589             |2024-05-01 20:08:44.309594|1983-08-13|porterfelicia@example.net |George Rivera     |2399138148            |\n",
            "|2511 Velasquez Lane Apt. 460\\nNicoleburgh, MP 76315|2024-05-02 23:05:44.150224|2019-07-18|jose81@example.com        |Nathan Thompson   |001-248-446-3967x92773|\n",
            "|4252 Hopkins Walks Apt. 146\\nMichaelfurt, VI 28228 |2024-05-01 13:30:02.298704|2014-09-19|perkinsbrandon@example.com|Catherine Shelton |(663)772-1452         |\n",
            "|0584 Hicks Road\\nHolmesville, MI 51824             |2024-05-02 05:10:20.287994|2015-06-10|katherinebrown@example.com|Brittany Rodriguez|(937)245-6806         |\n",
            "|910 Smith Club Suite 544\\nEast Amanda, MO 85261    |2024-05-01 14:13:07.924022|2013-05-31|sean92@example.com        |Nicholas Diaz     |(713)922-5139x94245   |\n",
            "|PSC 8012, Box 8614\\nAPO AE 72867                   |2024-05-02 11:00:37.092664|2013-05-29|melanie83@example.net     |Edwin Young       |784-244-3003          |\n",
            "|7962 Karen Circles Apt. 743\\nNorth Sonya, VI 70149 |2024-05-04 14:23:48.122863|1939-10-07|wfrancis@example.org      |Kara Snyder       |3956902737            |\n",
            "+---------------------------------------------------+--------------------------+----------+--------------------------+------------------+----------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from faker import Faker\n",
        "from datetime import datetime\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "users = []\n",
        "for _ in range(50):\n",
        "    user = {\n",
        "        'date': fake.date_time_between_dates(datetime(2024, 5, 1), datetime(2024, 5, 5)),\n",
        "        'name': fake.name(),\n",
        "        'address': fake.address(),\n",
        "        'email': fake.email(),\n",
        "        'dob': fake.date_of_birth(),\n",
        "        'phone': fake.phone_number()\n",
        "    }\n",
        "    users.append(user)\n",
        "\n",
        "df = spark.createDataFrame(users)\n",
        "\n",
        "df.show(10, False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGXjf6xpBj36"
      },
      "source": [
        "# Writing as PARQUET\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14stpbb4Bj37"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw5IIgebBj37",
        "outputId": "5a8585d9-b020-4005-a37b-97646ec1948c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-fb4cf8f3-a863-499a-bca1-6c995e9e9a84-c000.snappy.parquet  _SUCCESS\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Writing as PARQUET with no partitions\n",
        "\n",
        "path = \"/content/write_partitioning/parquet_no_partitions\"\n",
        "\n",
        "df.write.mode(\"overwrite\").format(\"parquet\").save(path)\n",
        "\n",
        "!ls /content/write_partitioning/parquet_no_partitions\n",
        "\n",
        "spark.read.format(\"parquet\").load(path).count() #indica o número de linhas que o parquet tem"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing as PARQUET with partitions\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "path = \"/content/write_partitioning/parquet_with_partitions\"\n",
        "\n",
        "# Creating partition column\n",
        "df = df.withColumn(\"date_part\", date_format(col(\"date\"), \"yyyyMMdd\"))\n",
        "\n",
        "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\") # enable dynamic partition overwrite - only overwrites partitions that are coming in the dataframe\n",
        "\n",
        "(df#.where(\"date_part = '20240503'\")\n",
        " .write\n",
        " .mode(\"overwrite\")                                               # overwrites the entire path with the new data\n",
        " .partitionBy(\"date_part\")                                        # partition the data by column - creates sub-folders for each partition - se queremos particionar tem mesmo que ser assim para garantir boa performace\n",
        " .format(\"parquet\")                                               # format of output\n",
        " .save(path))                                                     # path\n",
        "\n",
        "!ls /content/write_partitioning/parquet_with_partitions\n",
        "\n",
        "spark.read.format(\"parquet\").load(path).count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWX9WZbPHrL1",
        "outputId": "1b6bd84a-ecea-4fde-f686-b1d0d1b28015"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'date_part=20240501'  'date_part=20240502'  'date_part=20240503'  'date_part=20240504'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing as PARQUET with partitions by YEAR. MONTH, DAY\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "path = \"/content/write_partitioning/parquet_with_partitions_other\"\n",
        "\n",
        "# Creating partition column\n",
        "df = df\\\n",
        "    .withColumn(\"year\", date_format(col(\"date\"), \"yyyy\"))\\\n",
        "    .withColumn(\"month\", date_format(col(\"date\"), \"MM\"))\\\n",
        "    .withColumn(\"day\", date_format(col(\"date\"), \"dd\"))\n",
        "\n",
        "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\") # enable dynamic partition overwrite - only overwrites partitions that are coming in the dataframe\n",
        "\n",
        "(df#.where(\"date_part = '20240503'\")\n",
        " .write\n",
        " .mode(\"overwrite\")                                               # overwrites the entire path with the new data\n",
        " .partitionBy([\"year\", \"month\", \"day\"])                                        # partition the data by column - creates sub-folders for each partition - se queremos particionar tem mesmo que ser assim para garantir boa performace\n",
        " .format(\"parquet\")                                               # format of output\n",
        " .save(path))                                                     # path\n",
        "\n",
        "!ls /content/write_partitioning/parquet_with_partitions\n",
        "\n",
        "spark.read.format(\"parquet\").load(path).count()"
      ],
      "metadata": {
        "id": "ufswFldu6W4i",
        "outputId": "73279cd7-2af5-4115-9f29-1a14d4bd3b76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'date_part=20240501'  'date_part=20240502'  'date_part=20240503'  'date_part=20240504'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Duas formas diferentes de particionar:\n",
        "# /parquet/20240502\n",
        "\n",
        "# Spark way - hive-style\n",
        "# /parquet/date_part=20240502"
      ],
      "metadata": {
        "id": "DxwT7nJ75R-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking single partition\n",
        "spark.read.parquet(\"/content/write_partitioning/parquet_with_partitions/date_part=20240502\").show()\n",
        "\n",
        "\n",
        "# OU:\n",
        "delta = \"/content/write_partitioning/parquet_with_partitions/date_part=20240502\"\n",
        "spark.read.format(\"parquet\").load(delta).show()\n",
        "\n",
        "# OU:\n",
        "my_table = \"/content/write_partitioning/parquet_with_partitions/date_part=20240502\"\n",
        "spark.read.format(\"delta\").load(my_table).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0B62qu87JsAB",
        "outputId": "ef6218b6-2f9e-4278-f630-61523dea0a2b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+----------+--------------------+------------------+--------------------+\n",
            "|             address|                date|       dob|               email|              name|               phone|\n",
            "+--------------------+--------------------+----------+--------------------+------------------+--------------------+\n",
            "|USNS Conway\\nFPO ...|2024-05-02 22:32:...|2000-09-09|kathleenwhite@exa...|     Brian Ramirez|        547.878.6454|\n",
            "|2511 Velasquez La...|2024-05-02 23:05:...|2019-07-18|  jose81@example.com|   Nathan Thompson|001-248-446-3967x...|\n",
            "|0584 Hicks Road\\n...|2024-05-02 05:10:...|2015-06-10|katherinebrown@ex...|Brittany Rodriguez|       (937)245-6806|\n",
            "|PSC 8012, Box 861...|2024-05-02 11:00:...|2013-05-29|melanie83@example...|       Edwin Young|        784-244-3003|\n",
            "|360 Nathan Trail ...|2024-05-02 18:27:...|1968-04-15|avilajacob@exampl...|     Regina Miller|        798-255-9495|\n",
            "|7616 Harris Drive...|2024-05-02 04:11:...|1930-01-05|snydershawn@examp...|  Michael Thompson|   646-474-2913x4365|\n",
            "|473 Shelly Club S...|2024-05-02 21:02:...|1973-08-09|williampierce@exa...|       Patrick Lee|001-499-500-0577x420|\n",
            "|134 Hall Parkways...|2024-05-02 20:16:...|1962-06-16|caseyneal@example...|    Edward Francis| +1-296-287-9056x181|\n",
            "|946 Margaret Driv...|2024-05-02 18:28:...|1986-10-30|delgadojoel@examp...|   William Gilmore|001-688-344-6080x...|\n",
            "|1172 Nicholas Val...|2024-05-02 04:25:...|2023-06-13| thowell@example.org|    Andrea Aguirre|       (388)214-6422|\n",
            "|6645 Powell Cresc...|2024-05-02 12:42:...|1968-10-16|angelblack@exampl...|     Timothy Adams| (338)464-2669x17595|\n",
            "|631 Sandra Wall A...|2024-05-02 00:12:...|1944-02-29|ramirezcassie@exa...|       Derek Sharp|        532-320-5945|\n",
            "|PSC 6840, Box 025...|2024-05-02 17:51:...|2001-01-14|  xreyes@example.org|     John Richards|          4904968430|\n",
            "|13132 Timothy Inl...|2024-05-02 15:51:...|1989-07-02|donald95@example.com|      April Taylor|        315.520.2332|\n",
            "|64930 Audrey Vill...|2024-05-02 14:45:...|1909-08-16|rachel92@example.net| Lindsay Rodriguez|  (659)252-8450x8162|\n",
            "|03698 Matthew Isl...|2024-05-02 05:47:...|2007-02-07|debbie46@example.org|      Mary Edwards| +1-287-551-6580x780|\n",
            "|7610 Schultz Rue ...|2024-05-02 11:05:...|1956-10-16|walkersharon@exam...|       Shawn Ramos|   764-994-4647x8582|\n",
            "+--------------------+--------------------+----------+--------------------+------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+----------+--------------------+------------------+--------------------+\n",
            "|             address|                date|       dob|               email|              name|               phone|\n",
            "+--------------------+--------------------+----------+--------------------+------------------+--------------------+\n",
            "|USNS Conway\\nFPO ...|2024-05-02 22:32:...|2000-09-09|kathleenwhite@exa...|     Brian Ramirez|        547.878.6454|\n",
            "|2511 Velasquez La...|2024-05-02 23:05:...|2019-07-18|  jose81@example.com|   Nathan Thompson|001-248-446-3967x...|\n",
            "|0584 Hicks Road\\n...|2024-05-02 05:10:...|2015-06-10|katherinebrown@ex...|Brittany Rodriguez|       (937)245-6806|\n",
            "|PSC 8012, Box 861...|2024-05-02 11:00:...|2013-05-29|melanie83@example...|       Edwin Young|        784-244-3003|\n",
            "|360 Nathan Trail ...|2024-05-02 18:27:...|1968-04-15|avilajacob@exampl...|     Regina Miller|        798-255-9495|\n",
            "|7616 Harris Drive...|2024-05-02 04:11:...|1930-01-05|snydershawn@examp...|  Michael Thompson|   646-474-2913x4365|\n",
            "|473 Shelly Club S...|2024-05-02 21:02:...|1973-08-09|williampierce@exa...|       Patrick Lee|001-499-500-0577x420|\n",
            "|134 Hall Parkways...|2024-05-02 20:16:...|1962-06-16|caseyneal@example...|    Edward Francis| +1-296-287-9056x181|\n",
            "|946 Margaret Driv...|2024-05-02 18:28:...|1986-10-30|delgadojoel@examp...|   William Gilmore|001-688-344-6080x...|\n",
            "|1172 Nicholas Val...|2024-05-02 04:25:...|2023-06-13| thowell@example.org|    Andrea Aguirre|       (388)214-6422|\n",
            "|6645 Powell Cresc...|2024-05-02 12:42:...|1968-10-16|angelblack@exampl...|     Timothy Adams| (338)464-2669x17595|\n",
            "|631 Sandra Wall A...|2024-05-02 00:12:...|1944-02-29|ramirezcassie@exa...|       Derek Sharp|        532-320-5945|\n",
            "|PSC 6840, Box 025...|2024-05-02 17:51:...|2001-01-14|  xreyes@example.org|     John Richards|          4904968430|\n",
            "|13132 Timothy Inl...|2024-05-02 15:51:...|1989-07-02|donald95@example.com|      April Taylor|        315.520.2332|\n",
            "|64930 Audrey Vill...|2024-05-02 14:45:...|1909-08-16|rachel92@example.net| Lindsay Rodriguez|  (659)252-8450x8162|\n",
            "|03698 Matthew Isl...|2024-05-02 05:47:...|2007-02-07|debbie46@example.org|      Mary Edwards| +1-287-551-6580x780|\n",
            "|7610 Schultz Rue ...|2024-05-02 11:05:...|1956-10-16|walkersharon@exam...|       Shawn Ramos|   764-994-4647x8582|\n",
            "+--------------------+--------------------+----------+--------------------+------------------+--------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o206.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-d182bbc41ba9>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# OU:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmy_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/write_partitioning/parquet_with_partitions/date_part=20240502\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o206.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing as CSV\n",
        "\n",
        "https://spark.apache.org/docs/3.5.1/sql-data-sources-csv.html"
      ],
      "metadata": {
        "id": "n8mTC5yeNV6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnAWUTeZO43Z",
        "outputId": "39d06d44-835c-49cc-91c5-efe8243ec85e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/write_partitioning/csv_no_partitioning/\"\n",
        "\n",
        "# write as csv\n",
        "(df\n",
        "  .write\n",
        "  .format(\"csv\")\n",
        "  .mode(\"overwrite\")\n",
        "  .option(\"delimiter\", \"|\")\n",
        "  .option(\"header\", True)\n",
        "  .save(path))\n",
        "\n",
        "# listing files in the folder\n",
        "!ls /content/write_partitioning/csv_no_partitioning/\n",
        "\n",
        "# read as csv\n",
        "(spark\n",
        "  .read\n",
        "  .options(sep=\"|\", multiLine=True, header=True)\n",
        "  .csv(path)\n",
        "  .count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE6zC-HnNYAz",
        "outputId": "9ec500a7-4d7b-4c6f-bda8-60f01c88cab1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-c07c7f8d-ac57-4e64-84dc-af94dfe98aa2-c000.csv  _SUCCESS\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing as JSON\n",
        "\n",
        "https://spark.apache.org/docs/3.5.1/sql-data-sources-json.html"
      ],
      "metadata": {
        "id": "ZAuM5-WcTtyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/write_partitioning/json_no_partitioning/\"\n",
        "\n",
        "# write as json\n",
        "(df\n",
        ".write\n",
        ".mode(\"overwrite\")\n",
        ".format(\"json\")\n",
        ".save(path))\n",
        "\n",
        "# listing files in the folder\n",
        "!ls /content/write_partitioning/json_no_partitioning/\n",
        "\n",
        "# read as json\n",
        "(spark\n",
        "  .read\n",
        "  .json(path)\n",
        "  .count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnNgwbtxTsW_",
        "outputId": "99401d1e-028a-4085-dd12-cf4e563f8917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-4278fb78-1cc4-4622-9bb4-6cdadc50a8f2-c000.json  _SUCCESS\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading json as text\n",
        "spark.read.text(path).show(10, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3hYNCubT0ry",
        "outputId": "410f3f1f-d307-4736-e61d-74d2e916cf24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|value                                                                                                                                                                                                                                 |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|{\"address\":\"USCGC Garza\\nFPO AE 93586\",\"date\":\"2024-05-01T01:48:40.572Z\",\"dob\":\"1985-03-17\",\"email\":\"tinaperry@example.net\",\"name\":\"Connie Miller\",\"phone\":\"966-288-5241\",\"date_part\":\"20240501\"}                                     |\n",
            "|{\"address\":\"327 Harris Mall\\nRichstad, TX 25133\",\"date\":\"2024-05-02T11:43:30.828Z\",\"dob\":\"1939-06-04\",\"email\":\"timothy69@example.net\",\"name\":\"Bryan Cook\",\"phone\":\"(464)497-5312x67440\",\"date_part\":\"20240502\"}                       |\n",
            "|{\"address\":\"Unit 0282 Box 3495\\nDPO AA 47314\",\"date\":\"2024-05-04T00:33:05.895Z\",\"dob\":\"1992-07-30\",\"email\":\"qward@example.net\",\"name\":\"Crystal Smith\",\"phone\":\"(430)385-0871x4879\",\"date_part\":\"20240504\"}                            |\n",
            "|{\"address\":\"6337 Jennifer Center\\nTravischester, OH 54107\",\"date\":\"2024-05-04T03:27:51.258Z\",\"dob\":\"1997-04-23\",\"email\":\"pamelaperkins@example.com\",\"name\":\"Pamela Pena\",\"phone\":\"+1-913-489-6467\",\"date_part\":\"20240504\"}            |\n",
            "|{\"address\":\"8441 Diaz Mission\\nMartinhaven, MO 70679\",\"date\":\"2024-05-01T16:41:34.892Z\",\"dob\":\"1950-07-29\",\"email\":\"michellehansen@example.com\",\"name\":\"Crystal Edwards\",\"phone\":\"(616)980-5920x09352\",\"date_part\":\"20240501\"}        |\n",
            "|{\"address\":\"699 Troy Prairie Suite 319\\nKentchester, KY 01938\",\"date\":\"2024-05-02T11:30:46.459Z\",\"dob\":\"2018-12-31\",\"email\":\"megan98@example.net\",\"name\":\"Denise Sims\",\"phone\":\"(704)828-2189\",\"date_part\":\"20240502\"}                |\n",
            "|{\"address\":\"58674 Moore Camp Apt. 143\\nCampbellport, CO 62387\",\"date\":\"2024-05-02T20:05:50.653Z\",\"dob\":\"1990-07-13\",\"email\":\"hughesjessica@example.net\",\"name\":\"Susan Avila\",\"phone\":\"9717264698\",\"date_part\":\"20240502\"}             |\n",
            "|{\"address\":\"USNV Williams\\nFPO AE 37975\",\"date\":\"2024-05-03T20:40:49.066Z\",\"dob\":\"1962-07-06\",\"email\":\"christopher24@example.org\",\"name\":\"Jaime Baker\",\"phone\":\"584.910.7575\",\"date_part\":\"20240503\"}                                 |\n",
            "|{\"address\":\"53244 Cummings Parkway\\nShahtown, VT 73451\",\"date\":\"2024-05-03T14:59:18.526Z\",\"dob\":\"1921-02-15\",\"email\":\"sjohnson@example.net\",\"name\":\"Nicholas Smith\",\"phone\":\"001-568-975-4335x349\",\"date_part\":\"20240503\"}            |\n",
            "|{\"address\":\"8636 Brandon Knolls\\nLake Shaneborough, RI 71643\",\"date\":\"2024-05-01T21:47:14.124Z\",\"dob\":\"1980-10-29\",\"email\":\"nicoleanthony@example.net\",\"name\":\"Christopher Randall\",\"phone\":\"285.339.9877x754\",\"date_part\":\"20240501\"}|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading json as json\n",
        "spark.read.json(path).show(10, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bHcT2ilUo_F",
        "outputId": "b2ee72f5-23c1-4df0-f840-8caea3a5ee40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------+------------------------+---------+----------+--------------------------+-------------------+--------------------+\n",
            "|address                                          |date                    |date_part|dob       |email                     |name               |phone               |\n",
            "+-------------------------------------------------+------------------------+---------+----------+--------------------------+-------------------+--------------------+\n",
            "|USCGC Garza\\nFPO AE 93586                        |2024-05-01T01:48:40.572Z|20240501 |1985-03-17|tinaperry@example.net     |Connie Miller      |966-288-5241        |\n",
            "|327 Harris Mall\\nRichstad, TX 25133              |2024-05-02T11:43:30.828Z|20240502 |1939-06-04|timothy69@example.net     |Bryan Cook         |(464)497-5312x67440 |\n",
            "|Unit 0282 Box 3495\\nDPO AA 47314                 |2024-05-04T00:33:05.895Z|20240504 |1992-07-30|qward@example.net         |Crystal Smith      |(430)385-0871x4879  |\n",
            "|6337 Jennifer Center\\nTravischester, OH 54107    |2024-05-04T03:27:51.258Z|20240504 |1997-04-23|pamelaperkins@example.com |Pamela Pena        |+1-913-489-6467     |\n",
            "|8441 Diaz Mission\\nMartinhaven, MO 70679         |2024-05-01T16:41:34.892Z|20240501 |1950-07-29|michellehansen@example.com|Crystal Edwards    |(616)980-5920x09352 |\n",
            "|699 Troy Prairie Suite 319\\nKentchester, KY 01938|2024-05-02T11:30:46.459Z|20240502 |2018-12-31|megan98@example.net       |Denise Sims        |(704)828-2189       |\n",
            "|58674 Moore Camp Apt. 143\\nCampbellport, CO 62387|2024-05-02T20:05:50.653Z|20240502 |1990-07-13|hughesjessica@example.net |Susan Avila        |9717264698          |\n",
            "|USNV Williams\\nFPO AE 37975                      |2024-05-03T20:40:49.066Z|20240503 |1962-07-06|christopher24@example.org |Jaime Baker        |584.910.7575        |\n",
            "|53244 Cummings Parkway\\nShahtown, VT 73451       |2024-05-03T14:59:18.526Z|20240503 |1921-02-15|sjohnson@example.net      |Nicholas Smith     |001-568-975-4335x349|\n",
            "|8636 Brandon Knolls\\nLake Shaneborough, RI 71643 |2024-05-01T21:47:14.124Z|20240501 |1980-10-29|nicoleanthony@example.net |Christopher Randall|285.339.9877x754    |\n",
            "+-------------------------------------------------+------------------------+---------+----------+--------------------------+-------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# partition json data + saveAsTable\n",
        "\n",
        "# Creating partition column\n",
        "df = df.withColumn(\"date_part\", date_format(col(\"date\"), \"yyyyMMdd\"))\n",
        "\n",
        "# write as json\n",
        "(df.write\n",
        "  .partitionBy(\"date_part\")\n",
        "  .mode(\"overwrite\")\n",
        "  .format(\"json\")\n",
        "  .saveAsTable(\"tbl_json_part\"))\n",
        "\n",
        "# read as json\n",
        "spark.table(\"tbl_json_part\").count()\n",
        "\n",
        "# read as json\n",
        "spark.sql(\"show partitions tbl_json_part\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj59UNMuU0hV",
        "outputId": "6d80707f-d344-4f23-d7e3-161769ce9d96"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|         partition|\n",
            "+------------------+\n",
            "|date_part=20240501|\n",
            "|date_part=20240502|\n",
            "|date_part=20240503|\n",
            "|date_part=20240504|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Append Mode"
      ],
      "metadata": {
        "id": "6RhijzyqZeeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing as PARQUET with APPEND\n",
        "\n",
        "path = \"/content/write_partitioning/parquet_append\"\n",
        "\n",
        "df.write.mode(\"append\").format(\"parquet\").save(path)\n",
        "\n",
        "!ls /content/write_partitioning/parquet_append\n",
        "\n",
        "spark.read.format(\"parquet\").load(path).count()"
      ],
      "metadata": {
        "id": "GmLjA1zDZeG_",
        "outputId": "cd34dbf5-df38-40c0-b8c4-3f348705ba1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-0e63486c-ceb2-4091-8e36-87ec899929ec-c000.snappy.parquet\n",
            "part-00000-5112712e-2fd9-48c4-ba92-b97a63d00f2a-c000.snappy.parquet\n",
            "part-00000-dd17ce85-d320-4980-880c-a6f774e6efa7-c000.snappy.parquet\n",
            "_SUCCESS\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}