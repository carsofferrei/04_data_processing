{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carsofferrei/04_data_processing/blob/main/spark/challenges/challenges_CF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnUS6MsZbXtp",
        "outputId": "e3be18eb-9f95-4f2d-fe8b-2888f81bb5df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Spark session\n",
            "Starting ETL program\n",
            "Running Task - Ingestion\n",
            "Starting ingestion process for vehicles.\n",
            "Defining the schema for vehicles.\n",
            "Extracting from API\n",
            "Creating date column\n",
            "Loading on bronze layer\n",
            "Options on load: aggregate_file = True, saved in 1 files on folder and partitioned by date in parquet.\n",
            "Defining the schema for lines\n",
            "Extracting from API\n",
            "Loading on bronze layer\n",
            "Options on load: aggregate_file = True, saved in 1 files on folder and partitioned by None in parquet.\n",
            "Defining the schema for municipalities\n",
            "Extracting from API\n",
            "Loading on bronze layer\n",
            "Options on load: aggregate_file = True, saved in 1 files on folder and partitioned by None in parquet.\n",
            "Running Task - Cleansing\n",
            "Reading vehicles file from bronze layer\n",
            "Starting applying some transformations to the Dataframe\n",
            "Renaming some columns: lat - latitude and lon - longitude\n",
            "Removing duplicate records\n",
            "Removing records where CORRENT_STATUS is null\n",
            "Loading vehicles on silver layer\n",
            "Options on load: aggregate_file = True, saved in 1 files on folder and partitioned by date in parquet.\n",
            "Reading lines file from bronze layer\n",
            "Starting applying some transformations to the Dataframe\n",
            "Removing duplicate records\n",
            "Removing records where LONG_NAME is null\n",
            "Removing corrupt records\n",
            "Loading lines on silver layer\n",
            "Options on load: aggregate_file = True, saved in 1 files on folder and partitioned by None in parquet.\n",
            "Reading lines file from bronze layer\n",
            "Starting applying some transformations to the Dataframe\n",
            "Removing duplicate records\n",
            "Removing records where name OR district_name are null\n",
            "Loading lines on silver layer\n",
            "Options on load: aggregate_file = True, saved in 1 files on folder and partitioned by None in parquet.\n",
            "Running Task - Enrich\n",
            "Extracting info from the array column on lines DataFrame\n",
            "Joining vehicles with lines information\n",
            "Joining previous DataFrame with municipalities information\n",
            "Keep only some columns and remove duplicated records\n",
            "Removing records where name OR district_name are null\n",
            "Loading vehicles_enriched on gold layer\n",
            "Options on load: aggregate_file = True, saved in 1 files on folder and partitioned by date in parquet.\n",
            "ETL program completed\n",
            "Lets answers some questions:\n",
            "What are the top 3 municipalities by vehicles routes?\n",
            "The answers is: \n",
            "+------+\n",
            "|  name|\n",
            "+------+\n",
            "|Lisboa|\n",
            "|Sintra|\n",
            "|Almada|\n",
            "+------+\n",
            "\n",
            "What are the top 3 municipalities with higher speed on average?\n",
            "The answers is: \n",
            "+---------+\n",
            "|     name|\n",
            "+---------+\n",
            "|Alcochete|\n",
            "|  Montijo|\n",
            "|  Cascais|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import requests\n",
        "\n",
        "\n",
        "class ETLFlow:\n",
        "    def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "        self.spark.sparkContext.setLogLevel(\"INFO\")\n",
        "\n",
        "    def extract_from_file(self, format: str, path: str, **kwargs) -> DataFrame:\n",
        "        df = self.spark.read.format(format).load(path)\n",
        "        return df\n",
        "\n",
        "    def extract_from_api(self, url: str, schema: StructType = None):\n",
        "      response = requests.get(url)\n",
        "      rdd = spark.sparkContext.parallelize(response.json())\n",
        "      if schema:\n",
        "        df = spark.read.schema(schema).json(rdd)\n",
        "      else:\n",
        "        df = spark.read.json(rdd)\n",
        "      return df\n",
        "\n",
        "    def load(self, df: DataFrame, aggregate_file: bool = True, n_files: int = 1, partition_by: str = None, format: str = \"parquet\", path: str = None, **kwargs) -> None:\n",
        "      print(f\"Options on load: aggregate_file = {aggregate_file}, saved in {n_files} files on folder and partitioned by {partition_by} in {format}.\")\n",
        "      if aggregate_file:\n",
        "        if partition_by is not None:\n",
        "          df.coalesce(n_files).write.mode(\"overwrite\").partitionBy(partition_by).format(format).save(path)\n",
        "        else:\n",
        "          df.coalesce(n_files).write.mode(\"overwrite\").format(format).save(path)\n",
        "      else:\n",
        "        df.write.mode(\"overwrite\").format(format).save(path)\n",
        "\n",
        "\n",
        "class ETLTask(ETLFlow):\n",
        "\n",
        "    def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "    def ingestion_vehicles(self):\n",
        "      print(\"Starting ingestion process for vehicles.\")\n",
        "      print(\"Defining the schema for vehicles.\")\n",
        "      vehicle_schema = StructType([StructField('bearing', IntegerType(), True),\n",
        "                                  StructField('block_id', StringType(), True),\n",
        "                                  StructField('current_status', StringType(), True),\n",
        "                                  StructField('id', StringType(), True),\n",
        "                                  StructField('lat', FloatType(), True),\n",
        "                                  StructField('line_id', StringType(), True),\n",
        "                                  StructField('lon', FloatType(), True),\n",
        "                                  StructField('pattern_id', StringType(), True),\n",
        "                                  StructField('route_id', StringType(), True),\n",
        "                                  StructField('schedule_relationship', StringType(), True),\n",
        "                                  StructField('shift_id', StringType(), True),\n",
        "                                  StructField('speed', FloatType(), True),\n",
        "                                  StructField('stop_id', StringType(), True),\n",
        "                                  StructField('timestamp', TimestampType(), True),\n",
        "                                  StructField('trip_id', StringType(), True)])\n",
        "\n",
        "      print(f\"Extracting from API\")\n",
        "      df = self.extract_from_api(url=\"https://api.carrismetropolitana.pt/vehicles\", schema=vehicle_schema)\n",
        "      print(f\"Creating date column\")\n",
        "      df = df.withColumn(\"date\", date_format('timestamp', \"yyyyMMdd\"))\n",
        "      print(f\"Loading on bronze layer\")\n",
        "      self.load(df=df, aggregate_file = True, n_files = 1, partition_by = \"date\", format=\"parquet\", path=\"/content/lake/bronze/vehicles\")\n",
        "\n",
        "\n",
        "    def ingestion_lines(self):\n",
        "      print(f\"Defining the schema for lines\")\n",
        "      lines_schema = StructType([StructField('_corrupt_record', StringType(), True),\n",
        "                           StructField('color', StringType(), True),\n",
        "                           StructField('facilities', StringType(), True),\n",
        "                           StructField('id', StringType(), True),\n",
        "                           StructField('localities', ArrayType(StringType()), True),\n",
        "                           StructField('long_name', StringType(), True),\n",
        "                           StructField('municipalities', ArrayType(StringType()), True),\n",
        "                           StructField('patterns', ArrayType(StringType()), True),\n",
        "                           StructField('routes', StringType(), True),\n",
        "                           StructField('short_name', StringType(), True),\n",
        "                           StructField('text_color', StringType(), True)\n",
        "                           ])\n",
        "\n",
        "      print(f\"Extracting from API\")\n",
        "      df = self.extract_from_api(url=\"https://api.carrismetropolitana.pt/lines\", schema=lines_schema)\n",
        "      print(f\"Loading on bronze layer\")\n",
        "      self.load(df=df, format=\"parquet\", path=\"/content/lake/bronze/lines\")\n",
        "\n",
        "\n",
        "    def ingestion_municipalities(self):\n",
        "      print(f\"Defining the schema for municipalities\")\n",
        "      municipalities_schema = StructType([StructField('district_id', StringType(), True),\n",
        "                                    StructField('district_name', StringType(), True),\n",
        "                                    StructField('id', StringType(), True),\n",
        "                                    StructField('name', StringType(), True),\n",
        "                                    StructField('prefix', StringType(), True),\n",
        "                                    StructField('region_id', StringType(), True),\n",
        "                                    StructField('region_name', StringType(), True)\n",
        "                                    ])\n",
        "\n",
        "      print(f\"Extracting from API\")\n",
        "      df = self.extract_from_api(url=\"https://api.carrismetropolitana.pt/municipalities\", schema=municipalities_schema)\n",
        "      print(f\"Loading on bronze layer\")\n",
        "      self.load(df=df, format=\"parquet\", path=\"/content/lake/bronze/municipalities\")\n",
        "\n",
        "\n",
        "    def cleansing_vehicles(self, df: DataFrame):\n",
        "      print(f\"Reading vehicles file from bronze layer\")\n",
        "      df = self.extract_from_file(format=\"parquet\", path=\"/content/lake/bronze/vehicles\")\n",
        "\n",
        "      print(f\"Starting applying some transformations to the Dataframe\")\n",
        "      print(f\"Renaming some columns: lat - latitude and lon - longitude\")\n",
        "      df = df.withColumnRenamed(\"lat\", \"latitude\").withColumnRenamed(\"lon\", \"longitude\")\n",
        "\n",
        "      print(f\"Removing duplicate records\")\n",
        "      df = df.dropDuplicates()\n",
        "      print(f\"Removing records where CORRENT_STATUS is null\")\n",
        "      df = df.filter(df[\"current_status\"].isNotNull())\n",
        "\n",
        "      print(f\"Loading vehicles on silver layer\")\n",
        "      self.load(df=df, aggregate_file = True, n_files = 1, partition_by = \"date\", format=\"parquet\", path=\"/content/lake/silver/vehicles\")\n",
        "\n",
        "\n",
        "\n",
        "    def cleansing_lines(self, df: DataFrame):\n",
        "      print(f\"Reading lines file from bronze layer\")\n",
        "      df = self.extract_from_file(format=\"parquet\", path=\"/content/lake/bronze/lines\")\n",
        "\n",
        "\n",
        "      print(f\"Starting applying some transformations to the Dataframe\")\n",
        "      print(f\"Removing duplicate records\")\n",
        "      df = df.dropDuplicates()\n",
        "      print(f\"Removing records where LONG_NAME is null\")\n",
        "      df = df.filter(df[\"long_name\"].isNotNull())\n",
        "      print(f\"Removing corrupt records\")\n",
        "      df = df.filter(df[\"_corrupt_record\"].isNull())\n",
        "\n",
        "      print(f\"Loading lines on silver layer\")\n",
        "      self.load(df=df, format=\"parquet\", path=\"/content/lake/silver/lines\")\n",
        "\n",
        "\n",
        "\n",
        "    def cleansing_municipalities(self, df: DataFrame):\n",
        "      print(f\"Reading lines file from bronze layer\")\n",
        "      df = self.extract_from_file(format=\"parquet\", path=\"/content/lake/bronze/municipalities\")\n",
        "\n",
        "      print(f\"Starting applying some transformations to the Dataframe\")\n",
        "      print(f\"Removing duplicate records\")\n",
        "      df = df.dropDuplicates()\n",
        "      print(f\"Removing records where name OR district_name are null\")\n",
        "      df = df.filter((df[\"name\"].isNotNull()) | (df[\"district_name\"].isNotNull()))\n",
        "\n",
        "      print(f\"Loading lines on silver layer\")\n",
        "      self.load(df=df, format=\"parquet\", path=\"/content/lake/silver/municipalities\")\n",
        "\n",
        "\n",
        "\n",
        "    def enrich(self, path: str = \"/content/lake/silver\"):\n",
        "      vehicles = self.extract_from_file(format=\"parquet\", path = f\"{path}/vehicles\")\n",
        "      lines = self.extract_from_file(format=\"parquet\", path = f\"{path}/lines\")\n",
        "      municipalities = self.extract_from_file(format=\"parquet\", path = f\"{path}/municipalities\")\n",
        "\n",
        "      print(f\"Extracting info from the array column on lines DataFrame\")\n",
        "      lines_treated = lines.select(\"facilities\", \"id\", \"localities\", explode(lines.municipalities).alias(\"municipalities_id\")).dropDuplicates()\n",
        "      lines_treated = lines_treated.withColumnRenamed(\"id\", \"lines_id\")\n",
        "\n",
        "\n",
        "      print(f\"Joining vehicles with lines information\")\n",
        "      vehicles_lines = vehicles.join(lines_treated, vehicles['line_id'] == lines_treated['lines_id'], how = 'left')\n",
        "\n",
        "      print(f\"Joining previous DataFrame with municipalities information\")\n",
        "      municipalities = municipalities.withColumnRenamed(\"id\", \"id_municipalities\")\n",
        "      vehicles_enriched = vehicles_lines.join(municipalities, lines_treated['municipalities_id'] == municipalities['id_municipalities'], how = 'left')\n",
        "\n",
        "      print(f\"Keep only some columns and remove duplicated records\")\n",
        "      vvehicles_enriched = vehicles_enriched.select(\n",
        "                                                      \"line_id\"\n",
        "                                                    , \"current_status\"\n",
        "                                                    , \"schedule_relationship\"\n",
        "                                                    , \"shift_id\"\n",
        "                                                    , \"speed\"\n",
        "                                                    , \"stop_id\"\n",
        "                                                    , \"date\"\n",
        "                                                    , \"facilities\"\n",
        "                                                    , \"municipalities_id\"\n",
        "                                                    , \"district_name\"\n",
        "                                                    , \"name\"\n",
        "                                                    , \"prefix\"\n",
        "                                                    , \"region_id\"\n",
        "                                                    , \"region_name\"\n",
        "                                                ).dropDuplicates()\n",
        "\n",
        "\n",
        "      print(f\"Removing records where name OR district_name are null\")\n",
        "      vehicles_enriched = vehicles_enriched.filter((vehicles_enriched[\"name\"].isNotNull()) | (vvehicles_enriched[\"district_name\"].isNotNull()))\n",
        "\n",
        "\n",
        "      print(f\"Loading vehicles_enriched on gold layer\")\n",
        "      self.load(df = vehicles_enriched, aggregate_file = True, n_files = 1, partition_by = \"date\", format=\"parquet\", path=\"/content/lake/gold/vehicles_enriched\")\n",
        "\n",
        "\n",
        "    def answers(self):\n",
        "\n",
        "      vehicles_enriched = self.extract_from_file(format=\"parquet\", path = \"/content/lake/gold/vehicles_enriched\")\n",
        "\n",
        "      data_for_answers = vehicles_enriched.select(\"line_id\", \"name\", \"speed\").dropDuplicates()\n",
        "      data = data_for_answers.groupBy(\"name\").agg(\n",
        "                                                  count_distinct(\"line_id\").alias(\"count_line_ids\")\n",
        "                                                , round(sum(\"speed\"),2).alias(\"sum_speed\")\n",
        "                                                )\n",
        "\n",
        "      print(\"What are the top 3 municipalities by vehicles routes?\")\n",
        "      print(\"The answers is: \")\n",
        "      data.sort(data.count_line_ids.desc()).limit(3).select(\"name\").show()\n",
        "\n",
        "      print(\"What are the top 3 municipalities with higher speed on average?\")\n",
        "      print(\"The answers is: \")\n",
        "      data.withColumn(\"average_speed\",round((col(\"sum_speed\") / col(\"count_line_ids\")),2)).orderBy(col(\"average_speed\").desc()).limit(3).select(\"name\").show()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # init spark\n",
        "    from pyspark.sql import SparkSession\n",
        "    print(\"Initializing Spark session\")\n",
        "    spark = SparkSession.builder.master('local').appName('ETL Program').getOrCreate()\n",
        "\n",
        "    print(\"Starting ETL program\")\n",
        "    etl = ETLTask(spark)\n",
        "\n",
        "    # run tasks\n",
        "    print(\"Running Task - Ingestion\")\n",
        "    bronze_vehicles = etl.ingestion_vehicles()\n",
        "    bronze_lines = etl.ingestion_lines()\n",
        "    bronze_municipalities = etl.ingestion_municipalities()\n",
        "\n",
        "    print(\"Running Task - Cleansing\")\n",
        "    silver_vehicles = etl.cleansing_vehicles(df = bronze_vehicles)\n",
        "    silver_lines = etl.cleansing_lines(df = bronze_lines)\n",
        "    silver_municipalities = etl.cleansing_municipalities(df = bronze_municipalities)\n",
        "\n",
        "    print(\"Running Task - Enrich\")\n",
        "    etl.enrich()\n",
        "\n",
        "    print(\"ETL program completed\")\n",
        "\n",
        "    print(\"Lets answers some questions:\")\n",
        "    etl.answers()\n",
        "\n"
      ]
    }
  ]
}