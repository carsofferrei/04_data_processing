{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carsofferrei/04_data_processing/blob/main/spark_streaming/challenges/final_challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This script responds to the challenge for assessment in module #5-real-time-data:** Producer, read and write Stream on bronze layer and write Stream on silver layer applying some transformations. Also, have some Reports on data silver layer."
      ],
      "metadata": {
        "id": "gnUuroKzSfW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guarantee that don't have any other data on content folder\n",
        "!rm -rf content/lake"
      ],
      "metadata": {
        "id": "gw-U_MoTIEuE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "b2aca704-6912-463c-a2a3-cf842de8f80e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcybt71kTDNt"
      },
      "source": [
        "# Context\n",
        "Message events are coming from platform message broker (kafka, pubsub, kinesis...).\n",
        "You need to process the data according to the requirements.\n",
        "\n",
        "Message schema:\n",
        "- timestamp\n",
        "- value\n",
        "- event_type\n",
        "- message_id\n",
        "- country_id\n",
        "- user_id\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkyPORKNSYvV"
      },
      "source": [
        "# Challenge 1\n",
        "\n",
        "Step 1\n",
        "- Change exising producer\n",
        "\t- Change parquet location to \"/content/lake/bronze/messages/data\"\n",
        "\t- Add checkpoint (/content/lake/bronze/messages/checkpoint)\n",
        "\t- Delete /content/lake/bronze/messages and reprocess data\n",
        "\t- For reprocessing, run the streaming for at least 1 minute, then stop it\n",
        "\n",
        "Step 2\n",
        "- Implement new stream job to read from messages in bronze layer and split result in two locations\n",
        "\t- \"messages_corrupted\"\n",
        "\t\t- logic: event_status is null, empty or equal to \"NONE\"\n",
        "    - extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages_corrupted/data\n",
        "\n",
        "\t- \"messages\"\n",
        "\t\t- logic: not corrupted data\n",
        "\t\t- extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages/data\n",
        "\n",
        "\t- technical requirements\n",
        "\t\t- add checkpint (choose location)\n",
        "\t\t- use StructSchema\n",
        "\t\t- Set trigger interval to 5 seconds\n",
        "\t\t- run streaming for at least 20 seconds, then stop it\n",
        "\n",
        "\t- alternatives\n",
        "\t\t- implementing single streaming job with foreach/- foreachBatch logic to write into two locations\n",
        "\t\t- implementing two streaming jobs, one for messages and another for messages_corrupted\n",
        "\t\t- (paying attention on the paths and checkpoints)\n",
        "\n",
        "\n",
        "  - Check results:\n",
        "    - results from messages in bronze layer should match with the sum of messages+messages_corrupted in the silver layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udk3tohSaXOH",
        "outputId": "015b9624-d39e-46ff-a867-0d1905c3e8b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-33.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.17.0)\n",
            "Downloading Faker-33.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-33.1.0\n"
          ]
        }
      ],
      "source": [
        "%pip install faker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDGMKwBdi1qy"
      },
      "source": [
        "# Producer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPCOdivrfhYh",
        "outputId": "5fc57eb6-7d8e-424a-fc80-1b9c2b08603e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "fake = Faker()\n",
        "messages = [fake.uuid4() for _ in range(50)]\n",
        "\n",
        "def enrich_data(df, messages=messages):\n",
        "  fake = Faker()\n",
        "  new_columns = {\n",
        "      'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "      'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "      'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "      'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "      'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  return df\n",
        "\n",
        "def insert_messages(df: DataFrame, batch_id):\n",
        "  enrich = enrich_data(df)\n",
        "  enrich.write.mode(\"append\").format(\"parquet\").save(\"content/lake/bronze/messages/data\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# write stream\n",
        "query = (df_stream\n",
        "         .writeStream\n",
        "         .outputMode('append')\n",
        "         .option('checkpointLocation', \"content/lake/bronze/checkpoint\")\n",
        "         .trigger(processingTime='1 seconds')\n",
        "         .foreachBatch(insert_messages)\n",
        "         .start()\n",
        ")\n",
        "\n",
        "query.awaitTermination(60)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.isActive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpa6uFkUfUgJ",
        "outputId": "bac9253f-afb1-4091-ad1f-f105c1ea4a01"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KNyUK3yplDhg"
      },
      "outputs": [],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZWQExsnzlMFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a23c7c-1745-4f0d-d943-7dc7cb01117a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|           timestamp|value|event_type|          message_id|channel|country_id|user_id|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|2024-12-15 14:35:...|    0|  RECEIVED|7ecd39ae-02a7-46c...|    SMS|      2014|   1034|\n",
            "|2024-12-15 14:35:...|    2|  RECEIVED|7ecd39ae-02a7-46c...|    SMS|      2014|   1034|\n",
            "|2024-12-15 14:35:...|    4|  RECEIVED|7ecd39ae-02a7-46c...|    SMS|      2014|   1034|\n",
            "|2024-12-15 14:35:...|    1|  RECEIVED|7ecd39ae-02a7-46c...|    SMS|      2014|   1034|\n",
            "|2024-12-15 14:35:...|    3|  RECEIVED|7ecd39ae-02a7-46c...|    SMS|      2014|   1034|\n",
            "|2024-12-15 14:36:...|   80|  RECEIVED|20428c6e-b20a-440...|  EMAIL|      2011|   1001|\n",
            "|2024-12-15 14:35:...|   17|  RECEIVED|39e5d374-c360-447...|  EMAIL|      2001|   1004|\n",
            "|2024-12-15 14:35:...|   28|  RECEIVED|e7f3fbca-b7a7-423...|  EMAIL|      2014|   1040|\n",
            "|2024-12-15 14:36:...|   87|  RECEIVED|26c5f292-6c41-4d6...|  EMAIL|      2004|   1017|\n",
            "|2024-12-15 14:35:...|   44|  RECEIVED|b46c166d-9419-454...|  OTHER|      2010|   1048|\n",
            "|2024-12-15 14:35:...|   22|  RECEIVED|54b6cb6e-1331-438...|  OTHER|      2015|   1037|\n",
            "|2024-12-15 14:35:...|   31|  RECEIVED|94f8fe18-fbf7-419...|  EMAIL|      2005|   1027|\n",
            "|2024-12-15 14:35:...|   57|   CLICKED|b1b2368e-93b4-47a...|  EMAIL|      2006|   1022|\n",
            "|2024-12-15 14:35:...|   35|   CLICKED|8606735e-b843-4a8...|  EMAIL|      2008|   1004|\n",
            "|2024-12-15 14:35:...|    8|  RECEIVED|fc1452c5-ecea-4a8...|   PUSH|      2012|   1031|\n",
            "|2024-12-15 14:35:...|   39|  RECEIVED|12f24e77-9ce9-49d...|   CHAT|      2010|   1042|\n",
            "|2024-12-15 14:36:...|   84|   CREATED|b3cf387a-6e74-40e...|  EMAIL|      2013|   1006|\n",
            "|2024-12-15 14:35:...|   27|  RECEIVED|11c62a31-ffc0-407...|   CHAT|      2012|   1026|\n",
            "|2024-12-15 14:35:...|   40|   CREATED|caf29ac7-4ca4-409...|  EMAIL|      2002|   1050|\n",
            "|2024-12-15 14:35:...|   15|  RECEIVED|5d5dac79-aa5a-40c...|   CHAT|      2003|   1048|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/data\")\n",
        "df.show()\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraxHCycMdEZ"
      },
      "source": [
        "# Additional datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cfsus3dxMcQI"
      },
      "outputs": [],
      "source": [
        "countries = [\n",
        "    {\"country_id\": 2000, \"country\": \"Brazil\"},\n",
        "    {\"country_id\": 2001, \"country\": \"Portugal\"},\n",
        "    {\"country_id\": 2002, \"country\": \"Spain\"},\n",
        "    {\"country_id\": 2003, \"country\": \"Germany\"},\n",
        "    {\"country_id\": 2004, \"country\": \"France\"},\n",
        "    {\"country_id\": 2005, \"country\": \"Italy\"},\n",
        "    {\"country_id\": 2006, \"country\": \"United Kingdom\"},\n",
        "    {\"country_id\": 2007, \"country\": \"United States\"},\n",
        "    {\"country_id\": 2008, \"country\": \"Canada\"},\n",
        "    {\"country_id\": 2009, \"country\": \"Australia\"},\n",
        "    {\"country_id\": 2010, \"country\": \"Japan\"},\n",
        "    {\"country_id\": 2011, \"country\": \"China\"},\n",
        "    {\"country_id\": 2012, \"country\": \"India\"},\n",
        "    {\"country_id\": 2013, \"country\": \"South Korea\"},\n",
        "    {\"country_id\": 2014, \"country\": \"Russia\"},\n",
        "    {\"country_id\": 2015, \"country\": \"Argentina\"}\n",
        "]\n",
        "\n",
        "countries = spark.createDataFrame(countries)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_stream.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1rEAPuTUExX",
        "outputId": "a341b62d-41bb-42b0-dffc-5e95434cd326"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- value: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opo5H6rh099s",
        "outputId": "0530a043-cdd9-4ba8-ad31-28aae6ca9087"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- value: long (nullable = true)\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- message_id: string (nullable = true)\n",
            " |-- channel: string (nullable = true)\n",
            " |-- country_id: integer (nullable = true)\n",
            " |-- user_id: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swvPj9hVpzNf"
      },
      "source": [
        "# Streaming Messages x Messages Corrupted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import time\n",
        "\n",
        "def clean_message_events(df: DataFrame, batch_id):\n",
        "    if df.isEmpty():\n",
        "        print(\"No data in the current batch.\")\n",
        "        return\n",
        "\n",
        "    # Join with countries\n",
        "    df = df.join(countries, \"country_id\", \"left\")\n",
        "\n",
        "    # Filtering corrupted messages\n",
        "    corrupted_events = df.filter((col(\"event_type\").isNull() |\n",
        "                              (col(\"event_type\") == \"\") |\n",
        "                              (col(\"event_type\") == \"NONE\")\n",
        "                              ))\n",
        "\n",
        "    # Saving corrupted messages\n",
        "    corrupted_events.write.mode(\"append\").format(\"parquet\").partitionBy(\"date\").save(\"content/lake/silver/messages_corrupted/data\")\n",
        "\n",
        "    # Creating dataframe without corrupted messages\n",
        "    clean_events = df.filter(~(col(\"event_type\").isNull() |\n",
        "                          (col(\"event_type\") == \"\") |\n",
        "                          (col(\"event_type\") == \"NONE\"))\n",
        "                        )\n",
        "    # Saving clean messages\n",
        "    clean_events.write.mode(\"append\").format(\"parquet\").partitionBy(\"date\").save(\"content/lake/silver/messages/data\")\n",
        "\n",
        "\n",
        "print(f'Define streaming schema...')\n",
        "messages_schema = StructType([\n",
        "          StructField('timestamp', TimestampType(), True),\n",
        "          StructField('value', LongType(), True),\n",
        "          StructField('event_type', StringType(), True),\n",
        "          StructField('message_id', StringType(), True),\n",
        "          StructField('channel', StringType(), True),\n",
        "          StructField('country_id', IntegerType(), True),\n",
        "          StructField('user_id', IntegerType(), True)\n",
        "          ])\n",
        "\n",
        "print(f'Read the streaming data...')\n",
        "messages_event_data = spark.readStream.format(\"parquet\").schema(messages_schema).load(\"content/lake/bronze/messages/data/*\")\n",
        "\n",
        "print(f'Create a new date column that will be the split column in writeStreaming...')\n",
        "messages_event_data = messages_event_data.withColumn(\"date\", col('timestamp').cast(\"date\"))\n",
        "\n",
        "print(f'Write Streaming...')\n",
        "stream_silver_query = (messages_event_data\n",
        "                          .writeStream\n",
        "                          .outputMode('append')\n",
        "                          .option('checkpointLocation', 'content/lake/silver/checkpoint')\n",
        "                          .trigger(processingTime='5 seconds')\n",
        "                          .foreachBatch(clean_message_events)\n",
        "                          .start()\n",
        "                          )\n",
        "\n",
        "stream_silver_query.awaitTermination(20)"
      ],
      "metadata": {
        "id": "pY0wlHElmE5C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3de061db-2246-483d-e02a-62b86996c8b1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define streaming schema...\n",
            "Read the streaming data...\n",
            "Create a new date column that will be the split column in writeStreaming...\n",
            "Write Streaming...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stream_silver_query.isActive)\n",
        "print(stream_silver_query.stop())"
      ],
      "metadata": {
        "id": "FKEeTxroEoyr",
        "outputId": "71b1824b-e4bb-4e41-add1-c4bbf5c8b7f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLK9jpjCu3xE"
      },
      "source": [
        "## Checking data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nk8seEvbmvcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a998aae7-8794-4679-eb0f-32dd50f3aa61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation passed: Bronze [93] = Clean [69] + Corrupted [24]\n"
          ]
        }
      ],
      "source": [
        "def checking_silver_data():\n",
        "    bronze_count = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/data/*\").count()\n",
        "    clean_count = spark.read.format(\"parquet\").load(\"content/lake/silver/messages/data/*\").count()\n",
        "    corrupted_count = spark.read.format(\"parquet\").load(\"content/lake/silver/messages_corrupted/data/*\").count()\n",
        "\n",
        "    assert bronze_count == clean_count + corrupted_count, \"Dataframes doesn't matches. Did you run the code more than once? Be careful with append mode\"\n",
        "    print(f'Validation passed: Bronze [{bronze_count}] = Clean [{clean_count}] + Corrupted [{corrupted_count}]')\n",
        "\n",
        "checking_silver_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfxIlBISSvRP"
      },
      "source": [
        "# Challenge 2\n",
        "\n",
        "- Run business report\n",
        "- But first, there is a bug in the system which is causing some duplicated messages, we need to exclude these lines from the report\n",
        "\n",
        "- removing duplicates logic:\n",
        "  - Identify possible duplicates on message_id, event_type and channel\n",
        "  - in case of duplicates, consider only the first message (occurrence by timestamp)\n",
        "  - Ex:\n",
        "    In table below, the correct message to consider is the second line\n",
        "\n",
        "```\n",
        "    message_id | channel | event_type | timestamp\n",
        "    123        | CHAT    | CREATED    | 10:10:01\n",
        "    123        | CHAT    | CREATED    | 07:56:45 (first occurrence)\n",
        "    123        | CHAT    | CREATED    | 08:13:33\n",
        "```\n",
        "\n",
        "- After cleaning the data we're able to create the busines report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "R3J9XyOHhqvU"
      },
      "outputs": [],
      "source": [
        "# dedup data\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "df = spark.read.format(\"parquet\").load(\"content/lake/silver/messages\")\n",
        "dedup = df.withColumn(\"row_number\", F.row_number().over(Window.partitionBy(\"message_id\", \"event_type\", \"channel\").orderBy(\"timestamp\"))).filter(\"row_number = 1\").drop(\"row_number\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "silver_rows = df.count()\n",
        "dedup_rows = dedup.count()\n",
        "\n",
        "print(f'Clean messages data had {silver_rows - dedup_rows} dupplicated records.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnUBzL9TV679",
        "outputId": "e98374ed-adef-4ce3-d1b8-e35ce241344f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean messages data had 4 dupplicated records.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dedup.limit(3).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ5mMILDyEZ2",
        "outputId": "832fa9ed-269b-40bf-fd1f-32ec89f37c08"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+----------+--------------------+-------+-------+-------+----------+\n",
            "|country_id|           timestamp|value|event_type|          message_id|channel|user_id|country|      date|\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+-------+----------+\n",
            "|      2012|2024-12-15 14:36:...|   89|      SENT|09294331-a7c9-4c2...|  EMAIL|   1020|  India|2024-12-15|\n",
            "|      2005|2024-12-15 14:36:...|   78|   CREATED|11c62a31-ffc0-407...|  OTHER|   1009|  Italy|2024-12-15|\n",
            "|      2012|2024-12-15 14:35:...|   27|  RECEIVED|11c62a31-ffc0-407...|   CHAT|   1026|  India|2024-12-15|\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF9L9i25lk74"
      },
      "source": [
        "### Report 1\n",
        "  - Aggregate data by date, event_type and channel\n",
        "  - Count number of messages\n",
        "  - pivot event_type from rows into columns\n",
        "  - schema expected:\n",
        "  \n",
        "```\n",
        "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
        "+----------+-------+-------+-------+----+--------+----+\n",
        "|2024-12-03|    SMS|      4|      4|   1|       1|   5|\n",
        "|2024-12-03|   CHAT|      3|      7|   5|       8|   4|\n",
        "|2024-12-03|   PUSH|   NULL|      3|   4|       3|   4|\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UPHSMSXnTKgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8cdae1a-6b0a-4404-c20c-10e292c96990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining the pivot and expected schema\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|2024-12-15|  OTHER|      2|      2|   0|       2|   2|\n",
            "|2024-12-15|   PUSH|      4|      2|   1|       2|   1|\n",
            "|2024-12-15|  EMAIL|      3|      8|   1|       5|   4|\n",
            "|2024-12-15|    SMS|      3|      5|   1|       7|   4|\n",
            "|2024-12-15|   CHAT|      1|      1|   2|       5|   1|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f'Obtaining the pivot and expected schema')\n",
        "df.groupBy(\"date\", \"channel\").pivot(\"event_type\").agg(count(\"*\").alias(\"event_count\")).fillna(0).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxwOawo2lwQH"
      },
      "source": [
        "## Report 2\n",
        "\n",
        "- Identify the most active users by channel (sorted by number of iterations)\n",
        "- schema expected:\n",
        "\n",
        "```\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|   1022|         5|   2|    0|    1|   0|  2|\n",
        "|   1004|         4|   1|    1|    1|   1|  0|\n",
        "|   1013|         4|   0|    0|    2|   1|  1|\n",
        "|   1020|         4|   2|    0|    1|   1|  0|\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agg_int_total = df.groupBy(\"user_id\").agg(count(\"*\").alias(\"iterations\")).fillna(0)\n",
        "agg_channel_user = df.groupBy(\"user_id\").pivot(\"channel\").agg(count(\"*\").alias(\"iterations\")).fillna(0)\n",
        "agg_int_total.join(agg_channel_user, \"user_id\", \"left\").sort(desc(\"iterations\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpjCmSxxJY-M",
        "outputId": "f6c3f7d7-bd20-42c1-a317-35bb62c19670"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----+-----+-----+----+---+\n",
            "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
            "+-------+----------+----+-----+-----+----+---+\n",
            "|   1034|         6|   0|    0|    1|   0|  5|\n",
            "|   1020|         5|   0|    3|    0|   2|  0|\n",
            "|   1013|         4|   0|    1|    1|   1|  1|\n",
            "|   1022|         3|   0|    2|    0|   0|  1|\n",
            "|   1040|         3|   0|    1|    0|   0|  2|\n",
            "|   1004|         3|   0|    2|    1|   0|  0|\n",
            "|   1009|         3|   1|    0|    1|   1|  0|\n",
            "|   1031|         2|   0|    0|    0|   1|  1|\n",
            "|   1030|         2|   1|    1|    0|   0|  0|\n",
            "|   1021|         2|   0|    1|    0|   0|  1|\n",
            "|   1048|         2|   1|    0|    1|   0|  0|\n",
            "|   1002|         2|   0|    0|    0|   0|  2|\n",
            "|   1037|         2|   0|    0|    1|   0|  1|\n",
            "|   1015|         2|   0|    2|    0|   0|  0|\n",
            "|   1000|         2|   0|    1|    0|   0|  1|\n",
            "|   1039|         2|   1|    0|    0|   0|  1|\n",
            "|   1042|         2|   2|    0|    0|   0|  0|\n",
            "|   1027|         2|   0|    1|    1|   0|  0|\n",
            "|   1012|         2|   0|    0|    0|   0|  2|\n",
            "|   1047|         2|   1|    0|    0|   1|  0|\n",
            "+-------+----------+----+-----+-----+----+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_kzDbDwDOS"
      },
      "source": [
        "# Challenge 3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theoretical question:**\n",
        "\n",
        "*A new usecase requires the message data to be aggregate in near real time. They want to build a dashboard embedded in the platform website to analyze message data in low latency (few minutes). This application will access directly the data aggregated by streaming process.*\n",
        "\n",
        "- **Q1: What would be your suggestion to achieve that using Spark Structure Streaming? Or would you choose a different data processing tool?**\n",
        "- **A1:** By using Spark Struture Streaming, the possible solution would be:\n",
        "  - **1. Read stream data using Kafka format:** provides low-latency integration.\n",
        "  - **2. Add checkpoint:** ensure fault tolerance and state recovery in case of failure.\n",
        "  - **3. ATransform and aggregate data.**\n",
        "  - **4. Set a trigger rule to achieve near real-time updates.**\n",
        "  - **5. Write the result using memory** - used temporarily for low-latency processes.\n",
        "\n",
        " - If we have the possibility, another tool that can be applyed for this use case is Kafka Streams, because it allows real-time data reading, aggregation, and includes built-in support for dashboards and reporting.\n",
        "\n",
        "\n",
        "- **Q2: Which storage would you use and why? (database?, data lake?, kafka?)**\n",
        "\n",
        "- **A2:** All of them have strengths and weaknesses, and the choice depends on the requirements. If the business only needs real-time data and does not require historical preservation, Kafka meet the use case given the low-latency capabilities. However, if there is a need to preserve historical data and the flexibility to create different layers of data because requirements could change over time, a data lake is the better choice. It supports long-term storage and provides scalability for future needs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TK8sII0-0EL1"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}