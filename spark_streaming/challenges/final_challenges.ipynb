{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carsofferrei/04_data_processing/blob/main/spark_streaming/challenges/final_challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This script responds to the challenge for assessment in module #5-real-time-data:** Producer, read Stream for bronze and write Stream on silver applying some transformations. Also, have some Reports on data silver layer."
      ],
      "metadata": {
        "id": "gnUuroKzSfW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guarantee that don't have any other data on content folder\n",
        "!rm -rf content/lake"
      ],
      "metadata": {
        "id": "gw-U_MoTIEuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "70e1e562-5926-4440-9870-20101f8a42e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcybt71kTDNt"
      },
      "source": [
        "# Context\n",
        "Message events are coming from platform message broker (kafka, pubsub, kinesis...).\n",
        "You need to process the data according to the requirements.\n",
        "\n",
        "Message schema:\n",
        "- timestamp\n",
        "- value\n",
        "- event_type\n",
        "- message_id\n",
        "- country_id\n",
        "- user_id\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkyPORKNSYvV"
      },
      "source": [
        "# Challenge 1\n",
        "\n",
        "Step 1\n",
        "- Change exising producer\n",
        "\t- Change parquet location to \"/content/lake/bronze/messages/data\"\n",
        "\t- Add checkpoint (/content/lake/bronze/messages/checkpoint)\n",
        "\t- Delete /content/lake/bronze/messages and reprocess data\n",
        "\t- For reprocessing, run the streaming for at least 1 minute, then stop it\n",
        "\n",
        "Step 2\n",
        "- Implement new stream job to read from messages in bronze layer and split result in two locations\n",
        "\t- \"messages_corrupted\"\n",
        "\t\t- logic: event_status is null, empty or equal to \"NONE\"\n",
        "    - extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages_corrupted/data\n",
        "\n",
        "\t- \"messages\"\n",
        "\t\t- logic: not corrupted data\n",
        "\t\t- extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages/data\n",
        "\n",
        "\t- technical requirements\n",
        "\t\t- add checkpint (choose location)\n",
        "\t\t- use StructSchema\n",
        "\t\t- Set trigger interval to 5 seconds\n",
        "\t\t- run streaming for at least 20 seconds, then stop it\n",
        "\n",
        "\t- alternatives\n",
        "\t\t- implementing single streaming job with foreach/- foreachBatch logic to write into two locations\n",
        "\t\t- implementing two streaming jobs, one for messages and another for messages_corrupted\n",
        "\t\t- (paying attention on the paths and checkpoints)\n",
        "\n",
        "\n",
        "  - Check results:\n",
        "    - results from messages in bronze layer should match with the sum of messages+messages_corrupted in the silver layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udk3tohSaXOH",
        "outputId": "41374d06-29aa-4b3f-b7fc-ff4951d5f46e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-33.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.17.0)\n",
            "Downloading Faker-33.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.9 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.9 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-33.1.0\n"
          ]
        }
      ],
      "source": [
        "%pip install faker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDGMKwBdi1qy"
      },
      "source": [
        "# Producer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPCOdivrfhYh",
        "outputId": "9cfb6b03-0439-45ae-97a1-c1594f9cca10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "fake = Faker()\n",
        "messages = [fake.uuid4() for _ in range(50)]\n",
        "\n",
        "def enrich_data(df, messages=messages):\n",
        "  fake = Faker()\n",
        "  new_columns = {\n",
        "      'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "      'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "      'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "      'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "      'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  return df\n",
        "\n",
        "def insert_messages(df: DataFrame, batch_id):\n",
        "  enrich = enrich_data(df)\n",
        "  enrich.write.mode(\"append\").format(\"parquet\").save(\"content/lake/bronze/messages/data\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# write stream\n",
        "query = (df_stream\n",
        "         .writeStream\n",
        "         .outputMode('append')\n",
        "         .option('checkpointLocation', \"content/lake/bronze/checkpoint\")\n",
        "         .trigger(processingTime='1 seconds')\n",
        "         .foreachBatch(insert_messages)\n",
        "         .start()\n",
        ")\n",
        "\n",
        "query.awaitTermination(60)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.isActive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpa6uFkUfUgJ",
        "outputId": "d21ee4b2-3685-497d-b997-8aed587fee55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNyUK3yplDhg"
      },
      "outputs": [],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWQExsnzlMFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2e3a300-6025-4ef7-8292-07638afcebf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|           timestamp|value|event_type|          message_id|channel|country_id|user_id|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|2024-12-12 22:39:...|    1|      NONE|93043c1e-7ab5-4ce...|  EMAIL|      2014|   1029|\n",
            "|2024-12-12 22:39:...|    3|      NONE|93043c1e-7ab5-4ce...|  EMAIL|      2014|   1029|\n",
            "|2024-12-12 22:39:...|    5|      NONE|93043c1e-7ab5-4ce...|  EMAIL|      2014|   1029|\n",
            "|2024-12-12 22:39:...|    0|      NONE|93043c1e-7ab5-4ce...|  EMAIL|      2014|   1029|\n",
            "|2024-12-12 22:39:...|    2|      NONE|93043c1e-7ab5-4ce...|  EMAIL|      2014|   1029|\n",
            "|2024-12-12 22:39:...|    4|      NONE|93043c1e-7ab5-4ce...|  EMAIL|      2014|   1029|\n",
            "|2024-12-12 22:39:...|   13|  RECEIVED|5dfd6b47-4f83-4b7...|  OTHER|      2003|   1048|\n",
            "|2024-12-12 22:40:...|   66|  RECEIVED|6202eae8-0fa2-477...|  EMAIL|      2007|   1020|\n",
            "|2024-12-12 22:40:...|   58|  RECEIVED|3b306d82-d9f4-4ae...|  EMAIL|      2005|   1032|\n",
            "|2024-12-12 22:39:...|    7|  RECEIVED|42e57cf5-5752-48b...|  OTHER|      2003|   1046|\n",
            "|2024-12-12 22:40:...|   84|  RECEIVED|e8a3764c-c800-45c...|  EMAIL|      2007|   1018|\n",
            "|2024-12-12 22:39:...|   33|   CLICKED|6202eae8-0fa2-477...|  EMAIL|      2006|   1016|\n",
            "|2024-12-12 22:40:...|   92|  RECEIVED|613e3647-456c-479...|   CHAT|      2002|   1003|\n",
            "|2024-12-12 22:40:...|   57|   CREATED|31d43d67-643d-4f1...|  EMAIL|      2000|   1050|\n",
            "|2024-12-12 22:39:...|   29|  RECEIVED|a550dd8c-784e-47f...|   CHAT|      2008|   1048|\n",
            "|2024-12-12 22:40:...|   68|  RECEIVED|141c22b5-9401-48a...|   CHAT|      2010|   1020|\n",
            "|2024-12-12 22:40:...|   65|   CLICKED|9a593ca0-7094-4c8...|  EMAIL|      2004|   1031|\n",
            "|2024-12-12 22:39:...|   39|   CLICKED|b7c9dee0-6733-457...|  EMAIL|      2009|   1046|\n",
            "|2024-12-12 22:39:...|   32|   CLICKED|04f68b51-0c76-411...|  OTHER|      2009|   1038|\n",
            "|2024-12-12 22:39:...|   34|   CLICKED|9a593ca0-7094-4c8...|  EMAIL|      2012|   1039|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/data\")\n",
        "df.show()\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraxHCycMdEZ"
      },
      "source": [
        "# Additional datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfsus3dxMcQI"
      },
      "outputs": [],
      "source": [
        "countries = [\n",
        "    {\"country_id\": 2000, \"country\": \"Brazil\"},\n",
        "    {\"country_id\": 2001, \"country\": \"Portugal\"},\n",
        "    {\"country_id\": 2002, \"country\": \"Spain\"},\n",
        "    {\"country_id\": 2003, \"country\": \"Germany\"},\n",
        "    {\"country_id\": 2004, \"country\": \"France\"},\n",
        "    {\"country_id\": 2005, \"country\": \"Italy\"},\n",
        "    {\"country_id\": 2006, \"country\": \"United Kingdom\"},\n",
        "    {\"country_id\": 2007, \"country\": \"United States\"},\n",
        "    {\"country_id\": 2008, \"country\": \"Canada\"},\n",
        "    {\"country_id\": 2009, \"country\": \"Australia\"},\n",
        "    {\"country_id\": 2010, \"country\": \"Japan\"},\n",
        "    {\"country_id\": 2011, \"country\": \"China\"},\n",
        "    {\"country_id\": 2012, \"country\": \"India\"},\n",
        "    {\"country_id\": 2013, \"country\": \"South Korea\"},\n",
        "    {\"country_id\": 2014, \"country\": \"Russia\"},\n",
        "    {\"country_id\": 2015, \"country\": \"Argentina\"}\n",
        "]\n",
        "\n",
        "countries = spark.createDataFrame(countries)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_stream.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1rEAPuTUExX",
        "outputId": "b1665a7c-512c-40ba-da41-730ea5b7ce92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- value: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opo5H6rh099s",
        "outputId": "c69a78ea-b46b-441f-a453-af2c79e45da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- value: long (nullable = true)\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- message_id: string (nullable = true)\n",
            " |-- channel: string (nullable = true)\n",
            " |-- country_id: integer (nullable = true)\n",
            " |-- user_id: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swvPj9hVpzNf"
      },
      "source": [
        "# Streaming Messages x Messages Corrupted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import time\n",
        "\n",
        "def clean_message_events(df: DataFrame, batch_id):\n",
        "    if df.isEmpty():\n",
        "        print(\"No data in the current batch.\")\n",
        "        return\n",
        "\n",
        "    # Join with countries\n",
        "    df = df.join(countries, \"country_id\", \"left\")\n",
        "\n",
        "    # Filtering corrupted messages\n",
        "    corrupted_events = df.filter((col(\"event_type\").isNull() |\n",
        "                              (col(\"event_type\") == \"\") |\n",
        "                              (col(\"event_type\") == \"NONE\")\n",
        "                              ))\n",
        "\n",
        "    # Saving corrupted messages\n",
        "    corrupted_events.write.mode(\"append\").format(\"parquet\").partitionBy(\"date\").save(\"content/lake/silver/messages_corrupted/data\")\n",
        "\n",
        "    # Creating dataframe without corrupted messages\n",
        "    clean_events = df.filter(~(col(\"event_type\").isNull() |\n",
        "                          (col(\"event_type\") == \"\") |\n",
        "                          (col(\"event_type\") == \"NONE\"))\n",
        "                        )\n",
        "    # Saving clean messages\n",
        "    clean_events.write.mode(\"append\").format(\"parquet\").partitionBy(\"date\").save(\"content/lake/silver/messages/data\")\n",
        "\n",
        "\n",
        "print(f'Define streaming schema...')\n",
        "messages_schema = StructType([\n",
        "          StructField('timestamp', TimestampType(), True),\n",
        "          StructField('value', LongType(), True),\n",
        "          StructField('event_type', StringType(), True),\n",
        "          StructField('message_id', StringType(), True),\n",
        "          StructField('channel', StringType(), True),\n",
        "          StructField('country_id', IntegerType(), True),\n",
        "          StructField('user_id', IntegerType(), True)\n",
        "          ])\n",
        "\n",
        "print(f'Read the streaming data...')\n",
        "messages_event_data = spark.readStream.format(\"parquet\").schema(messages_schema).load(\"content/lake/bronze/messages/data/*\")\n",
        "\n",
        "print(f'Create a new date column that will be the split column in writeStreaming...')\n",
        "messages_event_data = messages_event_data.withColumn(\"date\", col('timestamp').cast(\"date\"))\n",
        "\n",
        "print(f'Write Streaming...')\n",
        "stream_silver_query = (messages_event_data\n",
        "                          .writeStream\n",
        "                          .outputMode('append')\n",
        "                          .option('checkpointLocation', 'content/lake/silver/checkpoint')\n",
        "                          .trigger(processingTime='5 seconds')\n",
        "                          .foreachBatch(clean_message_events)\n",
        "                          .start()\n",
        "                          )\n",
        "\n",
        "stream_silver_query.awaitTermination(20)"
      ],
      "metadata": {
        "id": "pY0wlHElmE5C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a851d715-2e7d-4f27-9384-47afbfbe7f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define streaming schema...\n",
            "Read the streaming data...\n",
            "Create a new date column that will be the split column in writeStreaming...\n",
            "Write Streaming...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stream_silver_query.isActive)\n",
        "print(stream_silver_query.stop())"
      ],
      "metadata": {
        "id": "FKEeTxroEoyr",
        "outputId": "090ca7ff-8d07-4b6a-c9bc-76f38093ed4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLK9jpjCu3xE"
      },
      "source": [
        "## Checking data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk8seEvbmvcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe82bfe3-ca09-40b3-c01f-bbada3dd4717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation passed: Bronze [96] = Clean [60] + Corrupted [36]\n"
          ]
        }
      ],
      "source": [
        "def checking_silver_data():\n",
        "    bronze_count = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/data/*\").count()\n",
        "    clean_count = spark.read.format(\"parquet\").load(\"content/lake/silver/messages/data/*\").count()\n",
        "    corrupted_count = spark.read.format(\"parquet\").load(\"content/lake/silver/messages_corrupted/data/*\").count()\n",
        "\n",
        "    assert bronze_count == clean_count + corrupted_count, \"Dataframes doesn't matches. Did you run the code more than once? Be careful with append mode\"\n",
        "    print(f'Validation passed: Bronze [{bronze_count}] = Clean [{clean_count}] + Corrupted [{corrupted_count}]')\n",
        "\n",
        "checking_silver_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfxIlBISSvRP"
      },
      "source": [
        "# Challenge 2\n",
        "\n",
        "- Run business report\n",
        "- But first, there is a bug in the system which is causing some duplicated messages, we need to exclude these lines from the report\n",
        "\n",
        "- removing duplicates logic:\n",
        "  - Identify possible duplicates on message_id, event_type and channel\n",
        "  - in case of duplicates, consider only the first message (occurrence by timestamp)\n",
        "  - Ex:\n",
        "    In table below, the correct message to consider is the second line\n",
        "\n",
        "```\n",
        "    message_id | channel | event_type | timestamp\n",
        "    123        | CHAT    | CREATED    | 10:10:01\n",
        "    123        | CHAT    | CREATED    | 07:56:45 (first occurrence)\n",
        "    123        | CHAT    | CREATED    | 08:13:33\n",
        "```\n",
        "\n",
        "- After cleaning the data we're able to create the busines report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3J9XyOHhqvU"
      },
      "outputs": [],
      "source": [
        "# dedup data\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "df = spark.read.format(\"parquet\").load(\"content/lake/silver/messages\")\n",
        "dedup = df.withColumn(\"row_number\", F.row_number().over(Window.partitionBy(\"message_id\", \"event_type\", \"channel\").orderBy(\"timestamp\"))).filter(\"row_number = 1\").drop(\"row_number\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "silver_rows = df.count()\n",
        "dedup_rows = dedup.count()\n",
        "\n",
        "print(f'Clean messages data had {silver_rows - dedup_rows} dupplicated records.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnUBzL9TV679",
        "outputId": "fd492604-c90f-4056-8431-0261f51bd0a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean messages data had 2 dupplicated records.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dedup.limit(3).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ5mMILDyEZ2",
        "outputId": "0ac35a7f-9eb5-48fd-9f96-1b88d37954ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+----------+--------------------+-------+-------+---------+----------+\n",
            "|country_id|           timestamp|value|event_type|          message_id|channel|user_id|  country|      date|\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+---------+----------+\n",
            "|      2015|2024-12-12 22:39:...|   18|   CLICKED|04f68b51-0c76-411...|  OTHER|   1030|Argentina|2024-12-12|\n",
            "|      2012|2024-12-12 22:39:...|   10|   CLICKED|04f68b51-0c76-411...|   PUSH|   1031|    India|2024-12-12|\n",
            "|      2011|2024-12-12 22:39:...|   16|      OPEN|04f68b51-0c76-411...|   CHAT|   1012|    China|2024-12-12|\n",
            "+----------+--------------------+-----+----------+--------------------+-------+-------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF9L9i25lk74"
      },
      "source": [
        "### Report 1\n",
        "  - Aggregate data by date, event_type and channel\n",
        "  - Count number of messages\n",
        "  - pivot event_type from rows into columns\n",
        "  - schema expected:\n",
        "  \n",
        "```\n",
        "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
        "+----------+-------+-------+-------+----+--------+----+\n",
        "|2024-12-03|    SMS|      4|      4|   1|       1|   5|\n",
        "|2024-12-03|   CHAT|      3|      7|   5|       8|   4|\n",
        "|2024-12-03|   PUSH|   NULL|      3|   4|       3|   4|\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPHSMSXnTKgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cd21958-9659-4eea-e033-cc8239dd7f96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining the pivot and expected schema\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|2024-12-12|   CHAT|      2|      3|   5|       3|   3|\n",
            "|2024-12-12|    SMS|      0|      2|   8|       1|   0|\n",
            "|2024-12-12|  EMAIL|      4|      2|   0|       3|   1|\n",
            "|2024-12-12|   PUSH|      5|      3|   1|       1|   2|\n",
            "|2024-12-12|  OTHER|      3|      3|   0|       2|   3|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f'Obtaining the pivot and expected schema')\n",
        "df.groupBy(\"date\", \"channel\").pivot(\"event_type\").agg(count(\"*\").alias(\"event_count\")).fillna(0).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxwOawo2lwQH"
      },
      "source": [
        "## Report 2\n",
        "\n",
        "- Identify the most active users by channel (sorted by number of iterations)\n",
        "- schema expected:\n",
        "\n",
        "```\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|   1022|         5|   2|    0|    1|   0|  2|\n",
        "|   1004|         4|   1|    1|    1|   1|  0|\n",
        "|   1013|         4|   0|    0|    2|   1|  1|\n",
        "|   1020|         4|   2|    0|    1|   1|  0|\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agg_int_total = df.groupBy(\"user_id\").agg(count(\"*\").alias(\"iterations\")).fillna(0)\n",
        "agg_channel_user = df.groupBy(\"user_id\").pivot(\"channel\").agg(count(\"*\").alias(\"iterations\")).fillna(0)\n",
        "agg_int_total.join(agg_channel_user, \"user_id\", \"left\").sort(desc(\"iterations\")).limit(4).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpjCmSxxJY-M",
        "outputId": "6ee6d7cd-24d6-41ce-f7c8-9d5b3434ff43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----+-----+-----+----+---+\n",
            "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
            "+-------+----------+----+-----+-----+----+---+\n",
            "|   1003|         5|   2|    0|    0|   1|  2|\n",
            "|   1018|         3|   1|    2|    0|   0|  0|\n",
            "|   1046|         3|   0|    1|    1|   0|  1|\n",
            "|   1020|         3|   1|    1|    1|   0|  0|\n",
            "+-------+----------+----+-----+-----+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_kzDbDwDOS"
      },
      "source": [
        "# Challenge 3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theoretical question:**\n",
        "\n",
        "*A new usecase requires the message data to be aggregate in near real time. They want to build a dashboard embedded in the platform website to analyze message data in low latency (few minutes). This application will access directly the data aggregated by streaming process.*\n",
        "\n",
        "- **Q1:**\n",
        "- **What would be your suggestion to achieve that using Spark Structure Streaming? Or would you choose a different data processing tool?**\n",
        "\n",
        "A1: By using Spark Struture Streaming, the possible solution would be:\n",
        "1. **Read Stream data applying kafka format** - Kafka alows real-time processing and integration with Kafka ensure low latency. \"Kafka is that it allows you to process large volume of data in real time and make near instantaneous decision based on the data\" (https://subhamkharwal.medium.com/pyspark-structured-streaming-read-from-kafka-64c40767155f)\n",
        "2. **Tranformations to the data as aggregation** - p.e., #topics per status, #topics per minute, some statistical measures on some variables on messages, etc..(depending on the requirments of the report)\n",
        "3. **Write the result using memory** - because we have the requirment of low-latency and the data will be already aggregated.\n",
        "\n",
        "The other tool that seems to be fair to use on this use case is Kafka Streams, beucase allows the reading, aggregation and have a dashboard/report implemented on the tool itself.\n",
        "(https://medium.com/syntio/data-processing-tools-overview-c333314ee1c5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- **Q2:**\n",
        "- **Which storage would you use and why? (database?, data lake?, kafka?)**\n",
        "\n",
        "A2: All of them have strengths and weaknesses, and the choice depends on the requirements. If the business only needs real-time data and does not require historical preservation, Kafka meet the use case given the low-latency capabilities.\n",
        "\n",
        "However, if there is a need to preserve historical data and the flexibility to create different layers of data because requirements could change over time, a data lake is the better choice. It supports long-term storage and provides scalability for future needs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TK8sII0-0EL1"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}