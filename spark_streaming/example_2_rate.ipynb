{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carsofferrei/04_data_processing/blob/main/spark_streaming/example_2_rate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA_wQSmLd9z"
      },
      "source": [
        "# Usecase 2\n",
        "- Reading data from \"rate\"\n",
        "- Aggregating data by window time\n",
        "- Checking results from query in memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "d1cf8eff-5b26-4374-8485-8fdec3f420de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "637HFw00T3LP"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master('local').appName('Test streaming').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write output in memory"
      ],
      "metadata": {
        "id": "NwzaZIoxqvrd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I4mGPfB-Xg_C"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "# read stream\n",
        "stream1 = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 10).load() #Escreve 10 ficheiros por segundo\n",
        "\n",
        "# transform\n",
        "transformed = stream1.withColumn(\"minute\", F.minute(\"timestamp\"))\n",
        "agg = transformed.groupBy(F.window(transformed.timestamp, \"5 seconds\")).count() #Intervalos de 5 em 5 segundos\n",
        "\n",
        "# write stream in memory\n",
        "query = (agg.writeStream\n",
        ".format('memory')\n",
        ".queryName('my_query')\n",
        ".outputMode('complete')\n",
        ".start()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmLQLr1uX6w-",
        "outputId": "0c3b9c59-65cb-4994-e843-7affde6d24e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+-----+\n",
            "|window                                    |count|\n",
            "+------------------------------------------+-----+\n",
            "|{2024-11-23 15:50:05, 2024-11-23 15:50:10}|30   |\n",
            "|{2024-11-23 15:50:00, 2024-11-23 15:50:05}|50   |\n",
            "|{2024-11-23 15:49:55, 2024-11-23 15:50:00}|50   |\n",
            "|{2024-11-23 15:49:50, 2024-11-23 15:49:55}|50   |\n",
            "|{2024-11-23 15:49:45, 2024-11-23 15:49:50}|50   |\n",
            "|{2024-11-23 15:49:40, 2024-11-23 15:49:45}|50   |\n",
            "|{2024-11-23 15:49:35, 2024-11-23 15:49:40}|10   |\n",
            "+------------------------------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"select * from my_query order by window desc\").show(10,False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TbLt4cUkX-JZ"
      },
      "outputs": [],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write output as parquet"
      ],
      "metadata": {
        "id": "Dc3r3j-wj16K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf content/output"
      ],
      "metadata": {
        "id": "N_BZRFCCpGq9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "v180mzIciVZH"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "def save_parquet(df, batch_id):\n",
        "  (df\n",
        "   .withColumn(\"batch_id\",F.lit(batch_id))\n",
        "   .withColumn(\"load_time\",F.current_timestamp())\n",
        "   .write.mode(\"append\")\n",
        "   .parquet(\"content/output/rate_parquet\")\n",
        "  )\n",
        "\n",
        "# read stream\n",
        "stream1 = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 10).load()\n",
        "\n",
        "# transform\n",
        "transformed = stream1.withWatermark(\"timestamp\", \"5 seconds\").withColumn(\"minute\", F.minute(\"timestamp\"))\n",
        "agg = transformed.groupBy(F.window(transformed.timestamp, \"5 seconds\")).count()\n",
        "\n",
        "# write stream as parquet with foreachBatch\n",
        "query = (agg.writeStream\n",
        ".option('checkpointLocation', 'content/output/checkpoint')\n",
        ".trigger(processingTime='20 seconds') # se esta janela for menor que os 5 segundos pode gerar duplicados\n",
        ".outputMode('append')\n",
        ".foreachBatch(save_parquet) # Dentro desta função passo a lógica de escrita através de um método\n",
        ".start()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = spark.read.format(\"parquet\").load(\"content/output/rate_parquet/\")\n",
        "result.sort(F.asc(\"window\")).show(100, False)\n",
        "\n",
        "# load_time - exatamente o momento em que escreveu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3SWIR6Ml8Al",
        "outputId": "16b98440-217a-44fc-ebc6-777e0a047299"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+-----+--------+--------------------------+\n",
            "|window                                    |count|batch_id|load_time                 |\n",
            "+------------------------------------------+-----+--------+--------------------------+\n",
            "|{2024-11-23 15:55:25, 2024-11-23 15:55:30}|14   |2       |2024-11-23 15:56:00.250838|\n",
            "|{2024-11-23 15:55:30, 2024-11-23 15:55:35}|50   |2       |2024-11-23 15:56:00.250838|\n",
            "|{2024-11-23 15:55:35, 2024-11-23 15:55:40}|50   |3       |2024-11-23 15:56:20.296245|\n",
            "|{2024-11-23 15:55:40, 2024-11-23 15:55:45}|50   |3       |2024-11-23 15:56:20.296245|\n",
            "|{2024-11-23 15:55:45, 2024-11-23 15:55:50}|50   |3       |2024-11-23 15:56:20.296245|\n",
            "|{2024-11-23 15:55:50, 2024-11-23 15:55:55}|50   |4       |2024-11-23 15:56:40.280023|\n",
            "|{2024-11-23 15:55:55, 2024-11-23 15:56:00}|50   |4       |2024-11-23 15:56:40.280023|\n",
            "|{2024-11-23 15:56:00, 2024-11-23 15:56:05}|50   |4       |2024-11-23 15:56:40.280023|\n",
            "|{2024-11-23 15:56:05, 2024-11-23 15:56:10}|50   |4       |2024-11-23 15:56:40.280023|\n",
            "|{2024-11-23 15:56:10, 2024-11-23 15:56:15}|50   |5       |2024-11-23 15:57:00.20245 |\n",
            "|{2024-11-23 15:56:15, 2024-11-23 15:56:20}|50   |5       |2024-11-23 15:57:00.20245 |\n",
            "|{2024-11-23 15:56:20, 2024-11-23 15:56:25}|50   |5       |2024-11-23 15:57:00.20245 |\n",
            "|{2024-11-23 15:56:25, 2024-11-23 15:56:30}|50   |5       |2024-11-23 15:57:00.20245 |\n",
            "|{2024-11-23 15:56:30, 2024-11-23 15:56:35}|50   |6       |2024-11-23 15:57:20.233146|\n",
            "|{2024-11-23 15:56:35, 2024-11-23 15:56:40}|50   |6       |2024-11-23 15:57:20.233146|\n",
            "|{2024-11-23 15:56:40, 2024-11-23 15:56:45}|50   |6       |2024-11-23 15:57:20.233146|\n",
            "|{2024-11-23 15:56:45, 2024-11-23 15:56:50}|50   |6       |2024-11-23 15:57:20.233146|\n",
            "|{2024-11-23 15:56:50, 2024-11-23 15:56:55}|50   |7       |2024-11-23 15:57:40.247186|\n",
            "|{2024-11-23 15:56:55, 2024-11-23 15:57:00}|50   |7       |2024-11-23 15:57:40.247186|\n",
            "|{2024-11-23 15:57:00, 2024-11-23 15:57:05}|50   |7       |2024-11-23 15:57:40.247186|\n",
            "|{2024-11-23 15:57:05, 2024-11-23 15:57:10}|50   |7       |2024-11-23 15:57:40.247186|\n",
            "|{2024-11-23 15:57:10, 2024-11-23 15:57:15}|50   |8       |2024-11-23 15:58:00.196542|\n",
            "|{2024-11-23 15:57:15, 2024-11-23 15:57:20}|50   |8       |2024-11-23 15:58:00.196542|\n",
            "|{2024-11-23 15:57:20, 2024-11-23 15:57:25}|50   |8       |2024-11-23 15:58:00.196542|\n",
            "|{2024-11-23 15:57:25, 2024-11-23 15:57:30}|50   |8       |2024-11-23 15:58:00.196542|\n",
            "+------------------------------------------+-----+--------+--------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "DKB-MAPOoEre",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f63b1c-85dd-4ec1-ef92-085d53063752"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
            "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/utils.py\", line 120, in call\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/utils.py\", line 117, in call\n",
            "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
            "  File \"<ipython-input-33-bd46483e2cb2>\", line 9, in save_parquet\n",
            "    .parquet(\"content/output/rate_parquet\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\", line 1721, in parquet\n",
            "    self._jwrite.parquet(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
            "    raise Py4JJavaError(\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o310.parquet.\n",
            ": java.lang.InterruptedException\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1040)\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1345)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:980)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
            "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy36.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enrich data with faker"
      ],
      "metadata": {
        "id": "GjAp1IKnvteX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "id": "_gauZX8MFP5f",
        "outputId": "e6820854-14aa-4cfd-e2c6-37688b02999a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-33.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Downloading Faker-33.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-33.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf content/output/events"
      ],
      "metadata": {
        "id": "UFND4p5-2Na5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "\n",
        "def insert_into_table(df, batch_id):\n",
        "  fake = Faker()\n",
        "  new_columns = {\n",
        "      'name': F.lit(fake.name()),\n",
        "      'address': F.lit(fake.address()),\n",
        "      'email': F.lit(fake.email()),\n",
        "      'dob': F.lit(fake.date_of_birth()),\n",
        "      'phone': F.lit(fake.phone_number())\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  df.write.mode(\"append\").format(\"parquet\").save(\"content/output/events\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# write stream\n",
        "query = (df_stream.writeStream\n",
        ".outputMode('append')\n",
        ".trigger(processingTime='1 seconds')\n",
        ".foreachBatch(insert_into_table)\n",
        ".start()\n",
        ")"
      ],
      "metadata": {
        "id": "WCUhAzDOD4Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()\n"
      ],
      "metadata": {
        "id": "KEMAlpKhwLNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.parquet(\"content/output/events\").show(100, False)"
      ],
      "metadata": {
        "id": "89s50dHjECqk",
        "outputId": "43972f9e-4221-44c9-e5a4-0de79fbff532",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+-----+------------------+-----------------------------------------------------------+----------------------------+----------+---------------------+\n",
            "|timestamp              |value|name              |address                                                    |email                       |dob       |phone                |\n",
            "+-----------------------+-----+------------------+-----------------------------------------------------------+----------------------------+----------+---------------------+\n",
            "|2024-11-22 17:09:40.928|13   |Jennifer Padilla  |4723 Justin Stravenue Apt. 795\\nSouth Ashleyville, CT 67628|michaeltorres@example.com   |1916-02-22|+1-868-292-8492x48606|\n",
            "|2024-11-22 17:09:43.928|16   |Brandon Camacho   |88045 Owens Points Apt. 983\\nEast Michelleborough, GU 99123|torresedward@example.net    |1932-09-15|(772)402-8764        |\n",
            "|2024-11-22 17:09:29.928|2    |Timothy Lopez     |311 Church Throughway Suite 666\\nDominguezbury, MH 74128   |gloverlaura@example.net     |1934-03-16|(507)746-9168        |\n",
            "|2024-11-22 17:09:30.928|3    |Jordan Goodwin PhD|Unit 7962 Box 6328\\nDPO AA 80830                           |christopher48@example.net   |1950-08-14|5635485892           |\n",
            "|2024-11-22 17:09:31.928|4    |Jordan Goodwin PhD|Unit 7962 Box 6328\\nDPO AA 80830                           |christopher48@example.net   |1950-08-14|5635485892           |\n",
            "|2024-11-22 17:09:35.928|8    |Teresa Kim        |141 Travis Hills Suite 933\\nNorth Arielview, OK 62545      |morrisdylan@example.net     |2022-12-24|549-905-3641x42100   |\n",
            "|2024-11-22 17:09:28.928|1    |Terry Ortiz       |42897 Jessica Drive Suite 379\\nDiaztown, OH 16669          |christinebaldwin@example.org|1926-06-02|(934)859-1870        |\n",
            "|2024-11-22 17:09:36.928|9    |William Garrison  |4319 Karen Junction\\nMcdonaldside, NM 00564                |powersdawn@example.org      |1975-02-21|(518)344-7585x1578   |\n",
            "|2024-11-22 17:09:32.928|5    |Jennifer Lloyd    |503 Nicole Flats\\nNorth Dustin, GA 38015                   |baxterbrittany@example.com  |1944-04-21|(388)669-7867x77982  |\n",
            "|2024-11-22 17:09:38.928|11   |Brandon Bradley   |314 Williams Turnpike\\nSouth Kaylaton, CA 03974            |ryanjensen@example.com      |2019-05-18|311.569.1608         |\n",
            "|2024-11-22 17:09:27.928|0    |Tracy Davis       |62231 Courtney Ridge\\nSouth Sally, NV 88848                |jasonwolf@example.net       |1947-07-03|+1-889-207-5787x65305|\n",
            "|2024-11-22 17:09:42.928|15   |Lisa Wilson       |9984 Jonathan Glens Suite 773\\nRodgersside, AL 84653       |molly78@example.com         |2020-03-12|7418347065           |\n",
            "|2024-11-22 17:09:37.928|10   |Heather Sims      |0115 Miller Court\\nPort Williamville, MA 26583             |ejones@example.com          |1923-04-20|724-666-4026x647     |\n",
            "|2024-11-22 17:09:34.928|7    |Jessica Wilson    |274 Martin Harbor\\nLake Lisa, GA 35484                     |margaretrandall@example.org |1977-09-15|(866)986-0391        |\n",
            "|2024-11-22 17:09:39.928|12   |Lindsay Henry     |44338 Ashley Trail\\nLarryborough, SC 30519                 |piercetracy@example.org     |1952-03-03|862-338-3227         |\n",
            "|2024-11-22 17:09:41.928|14   |George Smith      |213 Tyler Cliffs\\nWest Amandafurt, PW 58380                |brian63@example.com         |1931-07-12|360.255.7480         |\n",
            "|2024-11-22 17:09:33.928|6    |Amy Hamilton      |PSC 7718, Box 3814\\nAPO AP 62469                           |rachel31@example.com        |1955-11-11|753.276.1547x296     |\n",
            "+-----------------------+-----+------------------+-----------------------------------------------------------+----------------------------+----------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "m22vpxcxIUNa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}