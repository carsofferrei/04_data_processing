{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeAzxbINa4SWcnGdckKlfE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carsofferrei/04_data_processing/blob/main/spark_streaming/read_from_api_carris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HARxJHHN9yma"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "import requests\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.master('local').appName('Spark Course').config('spark.ui.port', '4050').getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"API Streaming\").getOrCreate()\n",
        "\n",
        "# Define schema for the JSON data\n",
        "vehicle_schema = StructType([\n",
        "    StructField('bearing', IntegerType(), True),\n",
        "    StructField('block_id', StringType(), True),\n",
        "    StructField('current_status', StringType(), True),\n",
        "    StructField('id', StringType(), True),\n",
        "    StructField('lat', FloatType(), True),\n",
        "    StructField('line_id', StringType(), True),\n",
        "    StructField('lon', FloatType(), True),\n",
        "    StructField('pattern_id', StringType(), True),\n",
        "    StructField('route_id', StringType(), True),\n",
        "    StructField('schedule_relationship', StringType(), True),\n",
        "    StructField('shift_id', StringType(), True),\n",
        "    StructField('speed', FloatType(), True),\n",
        "    StructField('stop_id', StringType(), True),\n",
        "    StructField('timestamp', TimestampType(), True),\n",
        "    StructField('trip_id', StringType(), True)\n",
        "])\n",
        "\n",
        "# API URL\n",
        "API_URL = \"https://api.carrismetropolitana.pt/vehicles\"\n",
        "\n",
        "# Directory to store streaming JSON files\n",
        "streaming_dir = \"/tmp/api_stream\"\n",
        "os.makedirs(streaming_dir, exist_ok=True)\n",
        "\n",
        "# Function to fetch API data and save it to JSON files\n",
        "def fetch_and_write_api_data(batch_id):\n",
        "    response = requests.get(API_URL)\n",
        "    if response.status_code == 200:\n",
        "        api_data = response.json()\n",
        "        with open(f\"{streaming_dir}/batch_{batch_id}.json\", \"w\") as f:\n",
        "            json.dump(api_data, f)\n",
        "        print(f\"Batch {batch_id} written to {streaming_dir}/batch_{batch_id}.json\")\n",
        "    else:\n",
        "        print(f\"Failed to fetch API data. Status: {response.status_code}\")\n",
        "\n",
        "# Periodically fetch API data\n",
        "def write_streaming_api_data(interval=5, max_batches=10):\n",
        "    batch_id = 0\n",
        "    while batch_id < max_batches:\n",
        "        fetch_and_write_api_data(batch_id)\n",
        "        time.sleep(interval)\n",
        "        batch_id += 1\n",
        "\n",
        "def save_parquet(df, batch_id):\n",
        "  (df\n",
        "   .withColumn(\"batch_id\",F.lit(batch_id))\n",
        "   .withColumn(\"load_time\",F.current_timestamp())\n",
        "   .write.mode(\"append\")\n",
        "   .parquet(\"content/output/api_carris_streaming\")\n",
        "  )\n",
        "\n",
        "\n",
        "# Start fetching API data in the background\n",
        "import threading\n",
        "threading.Thread(target=write_streaming_api_data).start()\n",
        "\n",
        "# Read the generated files as a streaming source\n",
        "df = spark.readStream.schema(vehicle_schema).json(streaming_dir)\n",
        "\n",
        "# Transform the streaming DataFrame\n",
        "transformed = df.withColumn(\"minute\", F.minute(\"timestamp\"))\n",
        "\n",
        "# Write the transformed stream to Parquet\n",
        "query = (transformed.writeStream\n",
        "            .option('checkpointLocation', 'content/output/checkpoint')\n",
        "            .trigger(processingTime='20 seconds')\n",
        "            .outputMode('append')\n",
        "            .foreachBatch(save_parquet)\n",
        "            .start()\n",
        "            )\n",
        "\n",
        "# Wait for the query to terminate\n",
        "query.awaitTermination(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcWbiluY_7hL",
        "outputId": "77321b14-9649-41fe-dd1d-1471e824a351"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 written to /tmp/api_stream/batch_0.json\n",
            "Batch 1 written to /tmp/api_stream/batch_1.json\n",
            "Batch 2 written to /tmp/api_stream/batch_2.json\n",
            "Batch 3 written to /tmp/api_stream/batch_3.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark.read.parquet(\"content/output/api_carris_streaming\").count())\n",
        "print(spark.read.parquet(\"content/output/api_carris_streaming\").dropDuplicates().count())\n",
        "\n",
        "print(\"Data have dupplicates. The number of records are:\")\n",
        "print((spark.read.parquet(\"content/output/api_carris_streaming\").count())-(spark.read.parquet(\"content/output/api_carris_streaming\").dropDuplicates().count()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asu4Y8f1C7TP",
        "outputId": "b471b0aa-5458-4616-8fd0-72c4f21dd982"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4200\n",
            "2734\n",
            "Data have dupplicates. The number of records are:\n",
            "1466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -rf \"/content/output/api_carris_streaming\""
      ],
      "metadata": {
        "id": "Vx9n42ytCIeZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oao83gQiDYi4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}